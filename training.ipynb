{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhU/5HTMTCvjyQUN3wFWbo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruttikajain/ACL2019-Reducing-Gender-Bias-in-Word-Level-Language-Models-Using-A-Gender-Equalizing-Loss-Function/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_3dkC6MpZsR",
        "outputId": "94bbab69-f6c8-4487-fc0f-517d8f4af80c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZzmUuPHsLNC",
        "outputId": "de202f92-e0eb-4e12-8da5-ffc06d81c0df"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks/MSProject"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/MSProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5j68nwdsWqN",
        "outputId": "c83fc8a5-507d-419d-98d9-ef227ca95e94"
      },
      "source": [
        "pip install import_ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLWHe6PQG1xw",
        "outputId": "6e5d8432-215b-4b58-db77-de9395c48a0b"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuBb3gaHpMdU"
      },
      "source": [
        "import import_ipynb\n",
        "import model\n",
        "#import trial"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcR8z7BlnMi1"
      },
      "source": [
        "import preprocess\n",
        "import data_v3"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_otAXeGCGPqk",
        "outputId": "7439c12a-ae77-4de7-fd62-11119310ec0d"
      },
      "source": [
        " #import datav3\n",
        "''' from google.colab import files\n",
        "\n",
        "uploaded = files.upload()'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' from google.colab import files\\n\\nuploaded = files.upload()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u7omYYITGU9J",
        "outputId": "3afb9948-f817-4696-a813-916ea8e53f06"
      },
      "source": [
        " #import preprocess\n",
        " '''from google.colab import files\n",
        "\n",
        "uploadedd = files.upload()'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from google.colab import files\\n\\nuploadedd = files.upload()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3n6KEbMwsI2"
      },
      "source": [
        "#args = parser.parse_args(args=[])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60eiIpYdVSks"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoQYE2n2WIE-",
        "outputId": "10ef8011-eba6-4aa8-8f19-8e8aef76702f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 22 03:29:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDTp0ytgX2aR",
        "outputId": "c7396435-f29f-483d-af52-16d46566ab7a"
      },
      "source": [
        "#### (1) Check if PyTorch and CUDA are available\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "#### (2) Inspect GPU\n",
        "!nvidia-smi\n",
        "#### (3) Test PyTorch\n",
        "import torch\n",
        "print(torch.ones(3,2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 22 03:29:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    12W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keMPrX80X4NO",
        "outputId": "6a2cd967-7cac-4329-cb43-59b783537430"
      },
      "source": [
        "!bash pytorch041_cuda92_colab.sh"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: pytorch041_cuda92_colab.sh: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96JtcJ3tXHgP",
        "outputId": "dc1549b8-3259-42fe-d5ac-c01de5f1db7d"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "directory /content/gdrive/My Drive/Colab Notebooks/MSProject/src already exists\n",
            "Out bin /content/gdrive/My Drive/Colab Notebooks/MSProject/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag4thum1hgY8",
        "outputId": "6212be35-630a-4a7d-84e5-a583c4236a9e"
      },
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import data_v3\n",
        "import model\n",
        "import preprocess\n",
        "#import jams\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "# seed = random.seed(20180330)\n",
        "#python -u  model/training_loss.py --dropout 0.25 --tied --lr 20 --anneal 4 --cuda --lamda 0 --glove --batch_size 48 --data ./data/preprocessed_50000/data --vocab ./data/preprocessed_50000/VOCAB.txt --save ./savedmodel/model_glove.pt --epochs 100 --glove_path ./glove/glove.txt\n",
        "#λ0.01 0.492 0.245 1.445 118.585 0.111 9.306 0.077\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
        "parser.add_argument('--data', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/Output/data',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=300,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=300,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=2,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=20,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--clip', type=float, default=0.25,\n",
        "                    help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=5,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=48, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=35,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0.2,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--lamda', type=float, default=1,\n",
        "                    help='loss parameter')\n",
        "parser.add_argument('--tied', action='store_true',\n",
        "                    help='tie the word embedding and softmax weights')\n",
        "parser.add_argument('--seed', type=int, default=20180330,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true',\n",
        "                    help='use CUDA', default=True)\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str,  default='/content/gdrive/My Drive/Colab Notebooks/MSProject/SavedModel/model.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--vocab', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/Output/VOCAB.txt',\n",
        "                    help=('preprocessed vocaburary'))\n",
        "parser.add_argument('--glove', action='store_true',\n",
        "                    help='use glove')\n",
        "parser.add_argument('--glove_path', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/glove.42B.300d.txt',\n",
        "                    help='using glove word embedding')\n",
        "parser.add_argument('--anneal', type=float, default=4,\n",
        "                    help='anneal rate of learning rate')\n",
        "#args = parser.parse_args()\n",
        "##args = parser.parse_args(args=[])\n",
        "#ap.parse_known_args()[0]\n",
        "args = parser.parse_args(args=[])\n",
        "#args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "    # else:\n",
        "    #     torch.cuda.manual_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "\n",
        "    return data.to(device)\n",
        "\n",
        "# 改动1\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def get_batch(source, i, evaluation=False):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
        "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
        "    return data, target\n",
        "\n",
        "\n",
        "#改动2\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output, targets, lamda, f_onehot, m_onehot)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        #loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss = criterion(output, targets, lamda, f_onehot, m_onehot)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
        "        ## this is deprecated, no need any more, just use torch.nn.utils.clip_grad_norm\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.data.add_(-lr, p.grad.data)\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss.item() / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "########################\n",
        "\n",
        "\n",
        "### LOSS CODE ###\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r', encoding='utf-8-sig')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "femaleFile = '/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/WordList/female_word_file.txt'\n",
        "maleFile = '/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/WordList/male_word_file.txt'\n",
        "\n",
        "def getGenderIdx(femaleFile, maleFile, word2idx):\n",
        "    female_word_list = load_doc(femaleFile).split('\\n')\n",
        "    male_word_list = load_doc(maleFile).split('\\n')\n",
        "    pairs = [ (word2idx[f],word2idx[m]) for f,m in zip(female_word_list,male_word_list)  if f in word2idx and m in word2idx]\n",
        "    femaleIdx = [ f for f,m in pairs]\n",
        "    maleIdx = [ m for f,m in pairs]\n",
        "    return femaleIdx,maleIdx\n",
        "\n",
        "class Custom_Loss(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Custom_Loss,self).__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def forward(self, output, targets, lamda, femaleIdx, maleIdx):\n",
        "        cross_ent_loss = self.criterion(output.view(-1, ntokens), targets)\n",
        "        bias_loss = self.logRatioLossFast(output, femaleIdx, maleIdx) * lamda\n",
        "        loss = cross_ent_loss + bias_loss     \n",
        "        return loss\n",
        "    \n",
        "    def customLoss_LogRatioGenderPairs(self,output, femaleIdx, maleIdx):\n",
        "        flato = output.view(-1, ntokens)\n",
        "        logratio_sum = 0\n",
        "        for i in range(flato.size()[0]):\n",
        "            o = flato[i]\n",
        "            logratio = [abs(torch.log( (torch.exp(o[f]) + 0.00001) / (torch.exp(o[m])+ 0.00001) )) for f,m in zip(femaleIdx, maleIdx)]\n",
        "            logratio_sum += sum(logratio)    \n",
        "        return logratio_sum / flato.size()[0]\n",
        "    \n",
        "    def logRatioLossFast(self, output, f_onehot, m_onehot):\n",
        "        flato = output.view(-1, ntokens)\n",
        "        m1 = torch.matmul(f_onehot,flato.t())\n",
        "        m2 = torch.matmul(m_onehot,flato.t())\n",
        "        b_loss = torch.mean(torch.abs(torch.log((torch.exp(m1) + 0.00001) / (torch.exp(m2) + 0.00001))))\n",
        "        return b_loss\n",
        "\n",
        "\n",
        "################\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "#load vocab            \n",
        "vocab = preprocess.read_vocab(os.path.join(args.vocab))\n",
        "#create json file with indexed filename for following separation\n",
        "inds = [os.path.join(args.data, x) for x in os.listdir(args.data) if x.endswith('bin')]\n",
        "# make new JSON file\n",
        "#with open('/content/drive/My Drive/Sandbox/Sandbox_2.json', 'w') as f:\n",
        "  #f.write('{\"hello\":true}')\n",
        "\n",
        "index_train = {}\n",
        "index_train['id'] = {}\n",
        "iteration = 0\n",
        "for ind in inds:\n",
        "    index_train['id'][iteration] = os.path.basename(ind)\n",
        "    iteration += 1\n",
        "with open('ind_train.json', 'w') as fp:\n",
        "    json.dump(index_train, fp)\n",
        "\n",
        "    \n",
        "#load the json file of indexed filename\n",
        "with open('ind_train.json', 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "idx_train_ = pd.DataFrame(data)\n",
        "\n",
        "print(idx_train_[:10])\n",
        "#split test set\n",
        "splitter_tt = ShuffleSplit(n_splits=1, test_size=0.1,\n",
        "                               random_state=args.seed)\n",
        "bigtrains, tests = next(splitter_tt.split(idx_train_['id'].keys()))\n",
        "\n",
        "idx_bigtrain = idx_train_.iloc[bigtrains]\n",
        "idx_test = idx_train_.iloc[tests]\n",
        "\n",
        "\n",
        "#split train, val sets\n",
        "splitter_tv = ShuffleSplit(n_splits=1, test_size=0.2,\n",
        "                               random_state=args.seed)\n",
        "\n",
        "trains, vals = next(splitter_tv.split(idx_bigtrain['id'].keys()))\n",
        "\n",
        "idx_train = idx_bigtrain.iloc[trains]\n",
        "idx_val = idx_bigtrain.iloc[vals]\n",
        "\n",
        "\n",
        "#save idx_train, idx_val, idx_test for later use\n",
        "idx_train.to_json('idx_train.json')\n",
        "idx_val.to_json('idx_val.json')\n",
        "idx_test.to_json('idx_test.json')\n",
        "\n",
        "\n",
        "# Load pretrained Embeddings, common token of vocab and gn_glove will be loaded, only appear in vocab will be initialized\n",
        "#142527 tokens, last one is '<unk>'\n",
        "# ntokens = sum(1 for line in open(gn_glove_dir)) + 1\n",
        "vocab.append('<eos>')\n",
        "ntokens = len(vocab)\n",
        "#\n",
        "\n",
        "with open(args.glove_path,'r+', encoding=\"utf-8\") as f: \n",
        "    gn_glove_vecs = np.zeros((142527, 300)) #april_1\n",
        "    words2idx_emb = {}\n",
        "    idx2words_emb = []\n",
        "    # ordered_words = []\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            s = line.split() \n",
        "            gn_glove_vecs[i, :] = np.asarray(s[1:])\n",
        "            words2idx_emb[s[0]] = i\n",
        "            idx2words_emb.append(s[0])\n",
        "            # ordered_words.append(s[0])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    words2idx_emb['<eos>'] = i+1\n",
        "    idx2words_emb.append('<eos>')\n",
        "\n",
        "#creat new word embeding, word embedding both  in the gn_glove and vocab keeps, only in vocab is initialized\n",
        "nw = np.zeros((ntokens, args.emsize), dtype=np.float32)\n",
        "\n",
        "for i,w in enumerate(vocab):#change add start=1\n",
        "    try:\n",
        "        r = words2idx_emb[w]\n",
        "        nw[i] = gn_glove_vecs[r] \n",
        "        test_i += 1 \n",
        "    except:\n",
        "        nw[i] = np.random.normal(scale=0.6, size=(args.emsize, ))\n",
        "\n",
        "words2idx = {item : index for index, item in enumerate(vocab)}\n",
        "# Load data\n",
        "corpus = data_v3.Corpus(args.data, vocab, words2idx, idx_train, idx_val, idx_test) #改动2 \n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "# nw = torch.from_numpy(nw)\n",
        "\n",
        "######################################\n",
        "\n",
        "femaleIdx,maleIdx = getGenderIdx(femaleFile, maleFile, corpus.words2idx)\n",
        "\n",
        "f_onehot = np.zeros((len(femaleIdx), ntokens))\n",
        "f_onehot[np.arange(len(femaleIdx)), femaleIdx] = 1\n",
        "f_onehot = torch.tensor(f_onehot, dtype = torch.float).to(device)\n",
        "\n",
        "m_onehot = np.zeros((len(femaleIdx), ntokens))\n",
        "m_onehot[np.arange(len(femaleIdx)), maleIdx] = 1\n",
        "m_onehot = torch.tensor(m_onehot, dtype = torch.float).to(device)\n",
        "\n",
        "\n",
        "\n",
        "###########################################\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = model.RNNModel(args.model, nw, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = Custom_Loss().to(device)\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "lamda = args.lamda\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if lr<1e-1 and lr>=1e-2:\n",
        "                lr /= (args.anneal/2)\n",
        "            elif lr<1e-2 and lr >=1e-3:\n",
        "                lr /= (args.anneal/3)\n",
        "            # elif lr<1e-4 and lr >=1e-5:\n",
        "            #     lr /= (args.anneal/3.5)\n",
        "            elif lr<1e-3:\n",
        "                lr *= 0.95\n",
        "            else:\n",
        "                lr /= args.anneal\n",
        "\n",
        "            print('new learning rate is {}'.format(lr))\n",
        "            print('val_loss is {}'.format(val_loss))\n",
        "            print('best_val_loss is {}'.format(best_val_loss))\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.3f} | test ppl {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(anneal=4, batch_size=20, bptt=35, clip=0.25, cuda=True, data='/content/gdrive/My Drive/Colab Notebooks/MSProject/Output/data', dropout=0.2, emsize=300, epochs=5, glove=False, glove_path='/content/gdrive/My Drive/Colab Notebooks/MSProject/glove.42B.300d.txt', lamda=1, log_interval=200, lr=20, model='LSTM', nhid=300, nlayers=2, save='/content/gdrive/My Drive/Colab Notebooks/MSProject/SavedModel/model.pt', seed=20180330, tied=False, vocab='/content/gdrive/My Drive/Colab Notebooks/MSProject/Output/VOCAB.txt')\n",
            "                                                id\n",
            "0     70630a9ce4576f9b93e759edfab6e90ffa6b16d1.bin\n",
            "1     20a49e016a4f3807fe24c3caee9af6bd9dc52b5b.bin\n",
            "10    08ecfda343d2405f633c0331eabc472f3dac4a26.bin\n",
            "100   8d3cb6429a30a810a8d281ee6e4b8bfc0c928446.bin\n",
            "1000  31171fd8e19677f021b50a335e490e3fb4680371.bin\n",
            "1001  0573daee6eff6931eb4ff49a599e80c74c20672e.bin\n",
            "1002  acbfc239711db1684963b4fe1ddd932e17763067.bin\n",
            "1003  8143b28f87a6e16d2b59aaf7725b6f4f3102b50f.bin\n",
            "1004  3a25f0552b90a5cb3be7e99edca8f83ade5bef9a.bin\n",
            "1005  4fccc492c45e8897c9bf23c4fb69f55679a5f5ff.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:139: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1715 batches | lr 20.00 | ms/batch 39.36 | loss  8.27 | ppl  3907.76\n",
            "| epoch   1 |   400/ 1715 batches | lr 20.00 | ms/batch 38.16 | loss  7.34 | ppl  1540.42\n",
            "| epoch   1 |   600/ 1715 batches | lr 20.00 | ms/batch 38.38 | loss  6.94 | ppl  1035.22\n",
            "| epoch   1 |   800/ 1715 batches | lr 20.00 | ms/batch 38.83 | loss  6.73 | ppl   840.07\n",
            "| epoch   1 |  1000/ 1715 batches | lr 20.00 | ms/batch 39.30 | loss  6.55 | ppl   697.58\n",
            "| epoch   1 |  1200/ 1715 batches | lr 20.00 | ms/batch 39.77 | loss  6.43 | ppl   622.86\n",
            "| epoch   1 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.26 | loss  6.38 | ppl   587.61\n",
            "| epoch   1 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.47 | loss  6.29 | ppl   541.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 75.04s | valid loss  6.17 | valid ppl   479.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1715 batches | lr 20.00 | ms/batch 40.69 | loss  6.23 | ppl   505.81\n",
            "| epoch   2 |   400/ 1715 batches | lr 20.00 | ms/batch 40.59 | loss  6.11 | ppl   451.90\n",
            "| epoch   2 |   600/ 1715 batches | lr 20.00 | ms/batch 40.65 | loss  6.07 | ppl   433.53\n",
            "| epoch   2 |   800/ 1715 batches | lr 20.00 | ms/batch 40.61 | loss  6.03 | ppl   415.71\n",
            "| epoch   2 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.59 | loss  5.95 | ppl   384.32\n",
            "| epoch   2 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.91 | ppl   369.45\n",
            "| epoch   2 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.90 | ppl   364.19\n",
            "| epoch   2 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.85 | ppl   348.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.44s | valid loss  5.89 | valid ppl   361.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1715 batches | lr 20.00 | ms/batch 40.78 | loss  5.85 | ppl   346.64\n",
            "| epoch   3 |   400/ 1715 batches | lr 20.00 | ms/batch 40.67 | loss  5.76 | ppl   317.12\n",
            "| epoch   3 |   600/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.74 | ppl   311.93\n",
            "| epoch   3 |   800/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.72 | ppl   305.22\n",
            "| epoch   3 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.67 | ppl   288.65\n",
            "| epoch   3 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.63 | ppl   279.10\n",
            "| epoch   3 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.59 | loss  5.62 | ppl   276.65\n",
            "| epoch   3 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.59 | ppl   267.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 77.46s | valid loss  5.76 | valid ppl   316.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1715 batches | lr 20.00 | ms/batch 40.89 | loss  5.61 | ppl   273.64\n",
            "| epoch   4 |   400/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.53 | ppl   251.01\n",
            "| epoch   4 |   600/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.52 | ppl   249.60\n",
            "| epoch   4 |   800/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.50 | ppl   244.72\n",
            "| epoch   4 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.46 | ppl   235.29\n",
            "| epoch   4 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.43 | ppl   228.76\n",
            "| epoch   4 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.42 | ppl   226.82\n",
            "| epoch   4 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.64 | loss  5.39 | ppl   220.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 77.48s | valid loss  5.70 | valid ppl   298.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1715 batches | lr 20.00 | ms/batch 40.77 | loss  5.42 | ppl   226.78\n",
            "| epoch   5 |   400/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.34 | ppl   209.30\n",
            "| epoch   5 |   600/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.34 | ppl   209.20\n",
            "| epoch   5 |   800/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.34 | ppl   207.53\n",
            "| epoch   5 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.30 | ppl   200.41\n",
            "| epoch   5 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.60 | loss  5.27 | ppl   195.18\n",
            "| epoch   5 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.65 | loss  5.26 | ppl   193.20\n",
            "| epoch   5 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.66 | loss  5.24 | ppl   187.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 77.47s | valid loss  5.67 | valid ppl   290.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 5.677 | test ppl  292.215\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbHjJdU8ww4I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "0d21383a-133f-4f59-a0ad-02e694c94a3c"
      },
      "source": [
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-953a85ef5553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_v3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_v3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}