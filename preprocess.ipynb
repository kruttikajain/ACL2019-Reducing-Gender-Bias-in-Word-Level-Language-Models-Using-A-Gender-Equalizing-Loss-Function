{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPi5w67gnalf61t1Aw+Tfl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruttikajain/ACL2019-Reducing-Gender-Bias-in-Word-Level-Language-Models-Using-A-Gender-Equalizing-Loss-Function/blob/master/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnG8KG_1Ejgz",
        "outputId": "3480cc41-b1c4-488d-8777-31ce018728bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JV80Nn0KoPG",
        "outputId": "679dd7db-67f7-482e-bd28-5717e8690871"
      },
      "source": [
        "\n",
        "%cd /content/gdrive/My Drive/M.S Project/Code/Loss"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/M.S Project/Code/Loss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcsIlCsxMsbU"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tna8YA0AKddH"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import re\n",
        "import ctypes\n",
        "import argparse\n",
        "import struct\n",
        "import pickle\n",
        "import gzip\n",
        "import spacy\n",
        "from unidecode import unidecode\n",
        "from io import BytesIO\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gc "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0BL2--YE25T"
      },
      "source": [
        "en = spacy.load('en')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4tkEQshFGrT"
      },
      "source": [
        "def is_valid_token(w):\n",
        "    \"\"\"\n",
        "    Returns True if a token is valid\n",
        "    \"\"\"\n",
        "    return bool(re.search('[a-zA-Z0-9,.!?<>\\']+', w))\n",
        "    #    !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pol8L8QXFGtk"
      },
      "source": [
        "def transform_token(w):\n",
        "    \"\"\"\n",
        "    Transforms a token by making lowercase, and for numeric tokens replaces\n",
        "    digits with placeholders\n",
        "    \"\"\"\n",
        "    #替换 非 A-Za-z <>$. - ‘，包括space\n",
        "    return re.sub(r'[^A-Za-z,.!?\\'<>]', '', \n",
        "            re.sub(r'\\d+', '<NUM>',\n",
        "                unidecode(w).lower()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi3OZk8VFGwR"
      },
      "source": [
        "# def is_valid_token(w):\n",
        "#     \"\"\"\n",
        "#     Returns True if a token is valid\n",
        "#     \"\"\"\n",
        "#     return bool(re.search('[a-zA-Z0-9,.!?]+', w))\n",
        "\n",
        "\n",
        "# def transform_token(w):\n",
        "#     \"\"\"\n",
        "#     Transforms a token by making lowercase, and for numeric tokens replaces\n",
        "#     digits with placeholders\n",
        "#     \"\"\"\n",
        "#     return re.sub(r'[.\\-\\']+$', '',\n",
        "#       re.sub(r'[.\\-\\']+', '',\n",
        "#         re.sub(r'[^A-Za-z<>$.\\-\\']', '',\n",
        "#             re.sub(r'\\d+', '<NUM>',\n",
        "#                 unidecode(w).lower()))))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgGccuaLFGym"
      },
      "source": [
        "def preprocess_file(filepath):\n",
        "    \"\"\"\n",
        "    Preprocesses a file by splitting it into sentences and tokenizing it\n",
        "    \"\"\"\n",
        "    # Open file\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            text = f.read()\n",
        "    except UnicodeDecodeError as e:\n",
        "        try:\n",
        "            # Account for some files that may be encoded with ISO-8859-1\n",
        "            with open(filepath, 'r', encoding='iso-8859-1') as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError as e:\n",
        "            msg = \"Could not open {}: {}\".format(filepath, str(e))\n",
        "            raise Exception(msg)\n",
        "\n",
        "\n",
        "    # Remove any additional information e.g. \"@highlights\"\n",
        "    main_text_body = text.split('\\n@')[0]\n",
        "\n",
        "    # Split up lines, and then break up lines into sentences\n",
        "    sentences = []\n",
        "    for line in main_text_body.split('\\n\\n'):\n",
        "        sentences += list(en(line.strip('\\n')).sents)\n",
        "\n",
        "    # Get tokens for each sentence\n",
        "    tokens = set()\n",
        "    sentence_tokens = []\n",
        "    for sent in sentences:\n",
        "        sent_tokens = []\n",
        "        for w in sent:\n",
        "            if not is_valid_token(w.text):\n",
        "                continue\n",
        "            w = transform_token(w.text)\n",
        "            sent_tokens.append(w)\n",
        "            tokens.add(w)\n",
        "        if len(sent_tokens) > 1:\n",
        "            sentence_tokens.append(sent_tokens)\n",
        "\n",
        "    return sentence_tokens, tokens"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ax8C06FG1R"
      },
      "source": [
        "def write_preprocessed_file(encoded_sentences, output_path):\n",
        "    \"\"\"\n",
        "    Write encoded sentences to a binary file\n",
        "    \"\"\"\n",
        "    num_sentences = len(encoded_sentences)\n",
        "    offset = 0\n",
        "    uint_size = ctypes.sizeof(ctypes.c_uint)\n",
        "\n",
        "    # Get total file size\n",
        "    total_size = sum([(len(sent)+1) * uint_size for sent in encoded_sentences]) - uint_size\n",
        "\n",
        "    buf = ctypes.create_string_buffer(total_size)\n",
        "    for sent_idx, sent in enumerate(encoded_sentences):\n",
        "        # Encode words as unsigned ints\n",
        "        for idx in sent:\n",
        "            struct.pack_into('I', buf, offset, idx + 1)\n",
        "            offset += uint_size\n",
        "\n",
        "        # Add a sentence delimter\n",
        "        if sent_idx != (num_sentences - 1):\n",
        "            struct.pack_into('I', buf, offset, 0)\n",
        "            offset += uint_size\n",
        "\n",
        "    # Gzip and save\n",
        "    with gzip.open(output_path, 'wb') as f:\n",
        "        f.write(buf)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMxKunxgFG3w"
      },
      "source": [
        "def encode_sentences(sentences, word_to_idx):\n",
        "    \"\"\"\n",
        "    Encode tokens in sentences by vocab indices\n",
        "    \"\"\"\n",
        "    tokenized = []\n",
        "    for sent in sentences:\n",
        "        sentence2idx = []\n",
        "        for w in sent:\n",
        "            try:\n",
        "                sentence2idx.append(word_to_idx[w])\n",
        "            except:\n",
        "                sentence2idx.append(word_to_idx['_unk_'])\n",
        "        tokenized.append(sentence2idx)\n",
        "\n",
        "    return tokenized\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcmHBn58FG56"
      },
      "source": [
        "def read_preprocessed_file(filepath, vocab):\n",
        "    \"\"\"\n",
        "    Reads a preprocessed text file. Returns a list of sentences, where\n",
        "    each sentence is a list of tokens.\n",
        "    \"\"\"\n",
        "    # Get binary string\n",
        "    with gzip.open(filepath, 'rb') as f:\n",
        "        buf = f.read()\n",
        "\n",
        "    sentences = []\n",
        "    sent = []\n",
        "    for (val,) in struct.iter_unpack('I', buf):\n",
        "        if val > 0:\n",
        "            # Get words for the current sentence\n",
        "            sent.append(vocab[val-1])\n",
        "        else:\n",
        "            # We've reached the end of the sentence\n",
        "            sentences.append(sent)\n",
        "            sent = []\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lsegs_VFG8M"
      },
      "source": [
        "def read_preprocessed_file_as_str(filepath, vocab, sent_delim='<eos>'):  #version2: no <eos> \n",
        "    \"\"\"\n",
        "    Reads a preprocessed text file. Returns a list of sentences, where\n",
        "    each sentence is a list of tokens.\n",
        "    \"\"\"\n",
        "    # Get binary string\n",
        "    with gzip.open(filepath, 'rb') as f:\n",
        "        buf = f.read()\n",
        "\n",
        "    res = \"\"\n",
        "    first_word = True\n",
        "    for (val,) in struct.iter_unpack('I', buf):\n",
        "        if not first_word:\n",
        "            res += ' '\n",
        "        else:\n",
        "            first_word = False\n",
        "\n",
        "        if val > 0:\n",
        "\n",
        "            # Get words for the current sentence\n",
        "            res += vocab[val-1]\n",
        "        else:\n",
        "            # We've reached the end of the sentence\n",
        "            res += sent_delim\n",
        "    res += ' ' + sent_delim\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def load_preprocesed_dataset(pp_dataset_dir, sent_delim='<eos>', vocab_path=None): #version2: no <eos> \n",
        "    if not vocab_path:\n",
        "        vocab_path = os.path.join(pp_dataset_dir, 'VOCAB_t.txt')\n",
        "\n",
        "    vocab = read_vocab(vocab_path)\n",
        "\n",
        "    data_dir = os.path.join(pp_dataset_dir, 'data/sample_stories')\n",
        "\n",
        "    res = \"\"\n",
        "    for idx, fname in enumerate(os.listdir(data_dir)):\n",
        "        if idx != 0:\n",
        "            res += ' '\n",
        "        filepath = os.path.join(data_dir, fname)\n",
        "        res += read_preprocessed_file_as_str(filepath, vocab, sent_delim='<eos>')\n",
        "\n",
        "    return res"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1CLaioqFG-U"
      },
      "source": [
        "def read_vocab(vocab_path):\n",
        "    \"\"\"\n",
        "    Read a vocabulary file. Returns a list of words\n",
        "    \"\"\"\n",
        "    vocab = []\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        for line in f:\n",
        "            vocab.append(line.strip('\\n'))\n",
        "\n",
        "    return vocab"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFbxczEXLPek"
      },
      "source": [
        "def preprocess_worker(args):\n",
        "    \"\"\"\n",
        "    Multiprocessing worker for preprocessing a text file\n",
        "    \"\"\"\n",
        "    txt_path, dataset_dir, output_data_dir = args\n",
        "    basename, ext = os.path.splitext(os.path.basename(txt_path))\n",
        "    out_prefix = os.path.dirname(txt_path).replace(dataset_dir, '').replace('/', '_')\n",
        "    if out_prefix:\n",
        "        out_prefix += '_'\n",
        "    out_path = os.path.join(output_data_dir, '{}{}.bin'.format(out_prefix, basename))\n",
        "\n",
        "    sentences, tokens = preprocess_file(txt_path)\n",
        "\n",
        "    return out_path, sentences, tokens"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W0F6UfDLPhI"
      },
      "source": [
        "\n",
        "def save_worker(args):\n",
        "    \"\"\"\n",
        "    Multiprocessing worker for saving a preprocessed file\n",
        "    \"\"\"\n",
        "    output_path, sentences, word_to_idx = args\n",
        "\n",
        "    sentences = encode_sentences(sentences, word_to_idx)\n",
        "    write_preprocessed_file(sentences, output_path)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKAczOuDLPjv"
      },
      "source": [
        "def preprocess_dataset(dataset_dir, output_dir, vocabsize, target_ext='.txt', num_workers=1):\n",
        "    \"\"\"\n",
        "    Preprocesses a dataset by splitting each file into sentences, tokenizing\n",
        "    each sentence, encoding the files, and saving them.\n",
        "    \"\"\"\n",
        "    dataset_dir = os.path.abspath(dataset_dir)\n",
        "    output_dir = os.path.abspath(output_dir)\n",
        "    output_data_dir = os.path.join(output_dir, 'data')\n",
        "    \n",
        "\n",
        "    if not os.path.isdir(dataset_dir):\n",
        "        raise ValueError('Dataset directory {} does not exist'.format(dataset_dir))\n",
        "\n",
        "    if not os.path.isdir(output_data_dir):\n",
        "        os.makedirs(output_data_dir)\n",
        "\n",
        "    worker_args = []\n",
        "\n",
        "    print(\"Getting list of files...\")\n",
        "    # Get list of txt files\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        root = os.path.abspath(root)\n",
        "        for fname in files:\n",
        "            basename, ext = os.path.splitext(fname)\n",
        "\n",
        "            # if ext.lower() != target_ext.lower():\n",
        "            #     continue\n",
        "\n",
        "            # if basename.lower() == 'readme':\n",
        "            #     continue\n",
        "\n",
        "            txt_path = os.path.join(root, fname)\n",
        "\n",
        "            worker_args.append((txt_path, dataset_dir, output_data_dir))\n",
        "\n",
        "    pool = mp.Pool(num_workers)\n",
        "\n",
        "    print(\"Preprocessing files...\")\n",
        "    output_paths = []\n",
        "    articles = []\n",
        "    wholetokens = []\n",
        "    num_files = len(worker_args)\n",
        "    # Preprocess each file and get the tokens in each file\n",
        "    for idx, (out_path, sentences, tokens) in enumerate(pool.imap_unordered(preprocess_worker, worker_args)):\n",
        "        output_paths.append(out_path)\n",
        "        articles.append(sentences)\n",
        "        wholetokens += sentences\n",
        "\n",
        "        if ((idx+1) % 1000) == 0:\n",
        "            print(\"Preprocessed {}/{} files\".format(idx+1, num_files))\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # create the tokenizer\n",
        "    voc_size = vocabsize #in keras,it builds a voc from 1-45000  original 47885\n",
        "    t = Tokenizer(num_words=voc_size,oov_token = '_unk_',filters='!\"$,-./:;<>?\\t\\n') \n",
        "    # fit the tokenizer on the documents\n",
        "    t.fit_on_texts(wholetokens)\n",
        "    # summarize what was learned\n",
        "    vocab = []\n",
        "    #in keras,t.word_index.items() is ordered dic, i.e {'work': 1, 'well': 2, 'done': 3, 'good': 4}\n",
        "    for key, value in t.word_index.items():\n",
        "        if value<voc_size:\n",
        "            vocab.append(key)\n",
        "        else:\n",
        "            break\n",
        "    vocab.insert(0, '_pad_')\n",
        "    print(len(vocab))\n",
        "\n",
        "    # Sort vocab and make into a list\n",
        "    print(\"Saving vocab...\")\n",
        "    vocab = list(sorted(vocab))\n",
        "    word_to_idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
        "\n",
        "    # Write vocab to disk\n",
        "    vocab_path = os.path.join(output_dir, 'VOCAB.txt')\n",
        "    with open(vocab_path, 'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    # Encode preprocessed files and write them to disk\n",
        "    worker_args = [(output_path, sentences, word_to_idx)\n",
        "                   for output_path, sentences in zip(output_paths, articles)]\n",
        "\n",
        "    print(\"Saving files...\")\n",
        "    pool = mp.Pool(num_workers)\n",
        "    for idx, _ in enumerate(pool.imap_unordered(save_worker, worker_args)):\n",
        "        if ((idx+1) % 1000) == 0:\n",
        "            print(\"Saved {}/{} files\".format(idx+1, num_files))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    print(\"Done.\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5PZ4hcULPmG"
      },
      "source": [
        "def parse_arguments():\n",
        "    \"\"\"\n",
        "    Get command line arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description='Preprocess text data into lists of tokens')\n",
        "    parser.add_argument('dataset_dir', help='Path to directory containing text files', type=str)\n",
        "    parser.add_argument('output_dir', help='Path to output directory', type=str)\n",
        "    parser.add_argument('target_ext', help='Extension of relevant text files', type=str)\n",
        "    parser.add_argument('vocabsize', help='Vocabulary size', type=int, default=50000)\n",
        "    parser.add_argument('-n', '--num-workers', dest='num_workers', type=int, default=1, help='Number of workers')\n",
        "    return vars(parser.parse_args())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "Kho9AMnSLPog",
        "outputId": "50320a70-c634-41bb-b3a9-e100b6815dee"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    preprocess_dataset(**(parse_arguments()))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-n NUM_WORKERS]\n",
            "                             dataset_dir output_dir target_ext vocabsize\n",
            "ipykernel_launcher.py: error: the following arguments are required: output_dir, target_ext, vocabsize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5yHkB5BLPqx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHGmyUk4FHAm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}