{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_cda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiGt4vQ+TLn3rwfjWX4+lX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruttikajain/ACL2019-Reducing-Gender-Bias-in-Word-Level-Language-Models-Using-A-Gender-Equalizing-Loss-Function/blob/master/training_cda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzATpNKmq0Lo",
        "outputId": "d002895f-e7a1-4f96-decb-31d77aeec350"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-5FySb0aX4z",
        "outputId": "56a8aa78-4921-4d3a-ddaf-a7ba0e435999"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Colab Notebooks/MSProject"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/MSProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahG2tho7akpx",
        "outputId": "4170bdd5-a209-4134-b327-d8a68d023bdf"
      },
      "source": [
        "pip install import_ipynb"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i27_uUbRakvQ",
        "outputId": "64832a02-8704-4557-91ec-891d0fe0a766"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v94kiYe_akxw"
      },
      "source": [
        "import import_ipynb"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hrHS7Veak0R"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5XbnBD2aX6_",
        "outputId": "5e51525d-9ec6-4b5f-b2ee-d665521c6a08"
      },
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import data_v3\n",
        "import model\n",
        "import preprocess\n",
        "#import jams\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "# seed = random.seed(20180330)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
        "parser.add_argument('--data', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/Preprocessed/CDA/data',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--model', type=str, default='LSTM',\n",
        "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
        "parser.add_argument('--emsize', type=int, default=300,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--nhid', type=int, default=300,\n",
        "                    help='number of hidden units per layer')\n",
        "parser.add_argument('--nlayers', type=int, default=2,\n",
        "                    help='number of layers')\n",
        "parser.add_argument('--lr', type=float, default=20,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--clip', type=float, default=0.25,\n",
        "                    help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=100,\n",
        "                    help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
        "                    help='batch size')\n",
        "parser.add_argument('--bptt', type=int, default=35,\n",
        "                    help='sequence length')\n",
        "parser.add_argument('--dropout', type=float, default=0.2,\n",
        "                    help='dropout applied to layers (0 = no dropout)')\n",
        "parser.add_argument('--lamda', type=float, default=1,\n",
        "                    help='loss parameter')\n",
        "parser.add_argument('--tied', action='store_true',\n",
        "                    help='tie the word embedding and softmax weights')\n",
        "parser.add_argument('--seed', type=int, default=20180330,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true', default=True,\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
        "                    help='report interval')\n",
        "parser.add_argument('--save', type=str,  default='/content/gdrive/My Drive/Colab Notebooks/MSProject/SavedModel/CDA/model.pt',\n",
        "                    help='path to save the final model')\n",
        "parser.add_argument('--vocab', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/Preprocessed/CDA/VOCAB.txt',\n",
        "                    help=('preprocessed vocaburary'))\n",
        "parser.add_argument('--glove', action='store_true',\n",
        "                    help='use glove')\n",
        "parser.add_argument('--glove_path', type=str, default='/content/gdrive/My Drive/Colab Notebooks/MSProject/glove.42B.300d.txt',\n",
        "                    help='using glove word embedding')\n",
        "parser.add_argument('--anneal', type=int, default=4,\n",
        "                    help='anneal rate of learing rate')\n",
        "#args = parser.parse_args()\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "print(args)\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "    # else:\n",
        "    #     torch.cuda.manual_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "\n",
        "    return data.to(device)\n",
        "\n",
        "# 改动1\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def get_batch(source, i, evaluation=False):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
        "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
        "    return data, target\n",
        "\n",
        "\n",
        "#改动2\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output, targets, lamda, f_onehot, m_onehot)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "#total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        #loss = criterion(output.view(-1, corpusns), targets)\n",
        "        loss = criterion(output, targets, lamda, f_onehot, m_onehot)\n",
        "        loss.backward()      \n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
        "        ## this is deprecated, no need any more, just use torch.nn.utils.clip_grad_norm\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.data.add_(-lr, p.grad.data)\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss.item() / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "########################\n",
        "\n",
        "\n",
        "### LOSS CODE ###\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r', encoding='utf-8-sig')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "#femaleFile = 'gender_words/female_word_file.txt'\n",
        "#maleFile = 'gender_words/male_word_file.txt'\n",
        "femaleFile = '/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/WordList/female_word_file.txt'\n",
        "maleFile = '/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/WordList/male_word_file.txt'\n",
        "\n",
        "def getGenderIdx(femaleFile, maleFile, word2idx):\n",
        "    female_word_list = load_doc(femaleFile).split('\\n')\n",
        "    male_word_list = load_doc(maleFile).split('\\n')\n",
        "    print(len(female_word_list),len(male_word_list))\n",
        "    pairs = [ (word2idx[f],word2idx[m]) for f,m in zip(female_word_list,male_word_list)  if f in word2idx and m in word2idx]\n",
        "    femaleIdx = [ f for f,m in pairs]\n",
        "    maleIdx = [ m for f,m in pairs]\n",
        "    print(len(femaleIdx),len(maleIdx))\n",
        "    return femaleIdx,maleIdx\n",
        "\n",
        "class Custom_Loss(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Custom_Loss,self).__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def forward(self, output, targets, lamda, femaleIdx, maleIdx):\n",
        "        cross_ent_loss = self.criterion(output.view(-1, ntokens), targets)\n",
        "        bias_loss = self.logRatioLossFast(output, femaleIdx, maleIdx) * lamda\n",
        "        #print((cross_ent_loss.item(),bias_loss.item()))\n",
        "        loss = cross_ent_loss + bias_loss     \n",
        "        return loss\n",
        "    \n",
        "    def customLoss_LogRatioGenderPairs(self,output, femaleIdx, maleIdx):\n",
        "        print(output.shape)\n",
        "        flato = output.view(-1, ntokens)\n",
        "        print(flato.shape)\n",
        "        logratio_sum = 0\n",
        "        for i in range(flato.size()[0]):\n",
        "            o = flato[i]\n",
        "            logratio = [abs(torch.log( (torch.exp(o[f]) + 0.00001) / (torch.exp(o[m])+ 0.00001) )) for f,m in zip(femaleIdx, maleIdx)]\n",
        "            #print([ a.item() for a in logratio])\n",
        "            #print([(o[f].item(),o[m].item()) for f,m in zip(femaleIdx, maleIdx)])\n",
        "            logratio_sum += sum(logratio)    \n",
        "        #print(logratio_sum.item())\n",
        "        return logratio_sum / flato.size()[0]\n",
        "    \n",
        "    def logRatioLossFast(self, output, f_onehot, m_onehot):\n",
        "        flato = output.view(-1, ntokens)\n",
        "        m1 = torch.matmul(f_onehot,flato.t())\n",
        "        m2 = torch.matmul(m_onehot,flato.t())\n",
        "        b_loss = torch.mean(torch.abs(torch.log((torch.exp(m1) + 0.00001) / (torch.exp(m2) + 0.00001))))\n",
        "        return b_loss\n",
        "\n",
        "\n",
        "################\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "#load vocab            \n",
        "vocab = preprocess.read_vocab(os.path.join(args.vocab))\n",
        "\n",
        "#calculate the data ids without swaped dataset\n",
        "inds = [os.path.join(args.data, x) for x in os.listdir(args.data) if x.endswith('bin') and len(x.split('.'))<3]\n",
        "inds_swap = [os.path.join(args.data, x) for x in os.listdir(args.data) if x.endswith('bin') and len(x.split('.'))==3]\n",
        "\n",
        "index_train = {}\n",
        "index_train['id'] = {}\n",
        "iteration = 0\n",
        "for ind in inds:\n",
        "    index_train['id'][iteration] = os.path.basename(ind)\n",
        "    iteration += 1\n",
        "with open('ind_train_cda.json', 'w') as fp:\n",
        "    json.dump(index_train, fp)\n",
        "\n",
        "    \n",
        "#load the json file of indexed filename\n",
        "with open('ind_train_cda.json', 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "idx_train_ = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "#split test set\n",
        "splitter_tt = ShuffleSplit(n_splits=1, test_size=0.1,\n",
        "                               random_state=args.seed)\n",
        "bigtrains, tests = next(splitter_tt.split(idx_train_['id'].keys()))\n",
        "\n",
        "idx_bigtrain = idx_train_.iloc[bigtrains]\n",
        "idx_test = idx_train_.iloc[tests]\n",
        "\n",
        "\n",
        "#split train, val sets\n",
        "splitter_tv = ShuffleSplit(n_splits=1, test_size=0.2,\n",
        "                               random_state=args.seed)\n",
        "\n",
        "trains, vals = next(splitter_tv.split(idx_bigtrain['id'].keys()))\n",
        "\n",
        "idx_train = idx_bigtrain.iloc[trains]\n",
        "idx_val = idx_bigtrain.iloc[vals]\n",
        "\n",
        "#add the swapped txts with same ids into training data\n",
        "idx_train_s = {}\n",
        "idx_train_s['id'] = {}\n",
        "\n",
        "ori_number = len(inds)\n",
        "ori_ids = [idx.split('.')[0] for idx in list(idx_train['id'])]\n",
        "\n",
        "for i, swap in enumerate(inds_swap):\n",
        "    if os.path.basename(swap).split('.')[0] in ori_ids:       \n",
        "        idx_train_s['id'][ori_number+i] = os.path.basename(swap)\n",
        "        \n",
        "idx_train_swaped = pd.DataFrame(idx_train_s)\n",
        "idx_train = pd.concat([idx_train, idx_train_swaped]).sample(frac=1,random_state=args.seed)\n",
        "\n",
        "#save idx_train, idx_val, idx_test for later use\n",
        "idx_train.to_json('idx_train_cda.json')\n",
        "idx_val.to_json('idx_val_cda.json')\n",
        "idx_test.to_json('idx_test_cda.json')\n",
        "\n",
        "\n",
        "# Load pretrained Embeddings, common token of vocab and gn_glove will be loaded, only appear in vocab will be initialized\n",
        "# ntokens = sum(1 for line in open(gn_glove_dir)) + 1\n",
        "vocab.append('<eos>')\n",
        "ntokens = len(vocab)\n",
        "#\n",
        "\n",
        "with open(args.glove_path,'r+', encoding=\"utf-8\") as f: \n",
        "    gn_glove_vecs = np.zeros((142527, 300)) #april_1\n",
        "    words2idx_emb = {}\n",
        "    idx2words_emb = []\n",
        "    # ordered_words = []\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            s = line.split() \n",
        "            gn_glove_vecs[i, :] = np.asarray(s[1:])\n",
        "            words2idx_emb[s[0]] = i\n",
        "            idx2words_emb.append(s[0])\n",
        "            # ordered_words.append(s[0])\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    words2idx_emb['<eos>'] = i+1\n",
        "    idx2words_emb.append('<eos>')\n",
        "\n",
        "#creat new word embeding, word embedding both  in the gn_glove and vocab keeps, only in vocab is initialized\n",
        "nw = np.zeros((ntokens, args.emsize), dtype=np.float32)\n",
        "\n",
        "for i,w in enumerate(vocab):#change add start=1\n",
        "    try:\n",
        "        r = words2idx_emb[w]\n",
        "        nw[i] = gn_glove_vecs[r] \n",
        "        test_i += 1 \n",
        "    except:\n",
        "        nw[i] = np.random.normal(scale=0.6, size=(args.emsize, ))\n",
        "\n",
        "words2idx = {item : index for index, item in enumerate(vocab)}\n",
        "# Load data\n",
        "corpus = data_v3.Corpus(args.data, vocab, words2idx, idx_train, idx_val, idx_test) #改动2 \n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "# nw = torch.from_numpy(nw)\n",
        "\n",
        "######################################\n",
        "\n",
        "femaleIdx,maleIdx = getGenderIdx(femaleFile, maleFile, corpus.words2idx)\n",
        "\n",
        "f_onehot = np.zeros((len(femaleIdx), ntokens))\n",
        "f_onehot[np.arange(len(femaleIdx)), femaleIdx] = 1\n",
        "f_onehot = torch.tensor(f_onehot, dtype = torch.float).to(device)\n",
        "\n",
        "m_onehot = np.zeros((len(femaleIdx), ntokens))\n",
        "m_onehot[np.arange(len(femaleIdx)), maleIdx] = 1\n",
        "m_onehot = torch.tensor(m_onehot, dtype = torch.float).to(device)\n",
        "\n",
        "###########################################\n",
        "\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = model.RNNModel(args.model, nw, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = Custom_Loss().to(device)\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "lamda = args.lamda\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            if lr<1e-1 and lr>=1e-2:\n",
        "                lr /= (args.anneal/2)\n",
        "            elif lr<1e-2 and lr >=1e-3:\n",
        "                lr /= (args.anneal/3)\n",
        "            elif lr<1e-3:\n",
        "                lr *= 0.95\n",
        "            else:\n",
        "                lr /= args.anneal\n",
        "\n",
        "            print('new learning rate is {}'.format(lr))\n",
        "            print('val_loss is {}'.format(val_loss))\n",
        "            print('best_val_loss is {}'.format(best_val_loss))\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.3f} | test ppl {:8.3f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(anneal=4, batch_size=20, bptt=35, clip=0.25, cuda=True, data='/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/Preprocessed/CDA/data', dropout=0.2, emsize=300, epochs=100, glove=False, glove_path='/content/gdrive/My Drive/Colab Notebooks/MSProject/glove.42B.300d.txt', lamda=1, log_interval=200, lr=20, model='LSTM', nhid=300, nlayers=2, save='/content/gdrive/My Drive/Colab Notebooks/MSProject/SavedModel/CDA/model.pt', seed=20180330, tied=False, vocab='/content/gdrive/My Drive/Colab Notebooks/MSProject/Data/Preprocessed/CDA/VOCAB.txt')\n",
            "224 224\n",
            "112 112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:139: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1715 batches | lr 20.00 | ms/batch 39.52 | loss  8.29 | ppl  3991.84\n",
            "| epoch   1 |   400/ 1715 batches | lr 20.00 | ms/batch 37.90 | loss  7.34 | ppl  1546.29\n",
            "| epoch   1 |   600/ 1715 batches | lr 20.00 | ms/batch 38.31 | loss  6.94 | ppl  1028.76\n",
            "| epoch   1 |   800/ 1715 batches | lr 20.00 | ms/batch 38.87 | loss  6.68 | ppl   797.76\n",
            "| epoch   1 |  1000/ 1715 batches | lr 20.00 | ms/batch 39.30 | loss  6.56 | ppl   705.37\n",
            "| epoch   1 |  1200/ 1715 batches | lr 20.00 | ms/batch 39.76 | loss  6.44 | ppl   626.81\n",
            "| epoch   1 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.46 | loss  6.37 | ppl   586.62\n",
            "| epoch   1 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.47 | loss  6.28 | ppl   532.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 75.02s | valid loss  6.14 | valid ppl   461.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1715 batches | lr 20.00 | ms/batch 40.75 | loss  6.23 | ppl   507.83\n",
            "| epoch   2 |   400/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  6.11 | ppl   452.55\n",
            "| epoch   2 |   600/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  6.05 | ppl   425.85\n",
            "| epoch   2 |   800/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.98 | ppl   394.62\n",
            "| epoch   2 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.59 | loss  5.96 | ppl   388.60\n",
            "| epoch   2 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.63 | loss  5.91 | ppl   367.53\n",
            "| epoch   2 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.63 | loss  5.89 | ppl   362.91\n",
            "| epoch   2 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.85 | ppl   346.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.31s | valid loss  5.86 | valid ppl   351.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1715 batches | lr 20.00 | ms/batch 40.78 | loss  5.84 | ppl   343.92\n",
            "| epoch   3 |   400/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.76 | ppl   315.77\n",
            "| epoch   3 |   600/ 1715 batches | lr 20.00 | ms/batch 40.54 | loss  5.71 | ppl   302.91\n",
            "| epoch   3 |   800/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.67 | ppl   289.05\n",
            "| epoch   3 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.66 | loss  5.67 | ppl   288.60\n",
            "| epoch   3 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.62 | loss  5.63 | ppl   278.13\n",
            "| epoch   3 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.62 | ppl   275.12\n",
            "| epoch   3 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.59 | ppl   268.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 77.33s | valid loss  5.75 | valid ppl   313.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1715 batches | lr 20.00 | ms/batch 40.78 | loss  5.59 | ppl   267.43\n",
            "| epoch   4 |   400/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.52 | ppl   250.40\n",
            "| epoch   4 |   600/ 1715 batches | lr 20.00 | ms/batch 40.61 | loss  5.49 | ppl   241.71\n",
            "| epoch   4 |   800/ 1715 batches | lr 20.00 | ms/batch 40.62 | loss  5.45 | ppl   233.05\n",
            "| epoch   4 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.60 | loss  5.46 | ppl   234.47\n",
            "| epoch   4 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.43 | ppl   227.24\n",
            "| epoch   4 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.42 | ppl   226.52\n",
            "| epoch   4 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.40 | ppl   222.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 77.35s | valid loss  5.68 | valid ppl   293.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1715 batches | lr 20.00 | ms/batch 40.76 | loss  5.40 | ppl   221.60\n",
            "| epoch   5 |   400/ 1715 batches | lr 20.00 | ms/batch 40.62 | loss  5.35 | ppl   210.89\n",
            "| epoch   5 |   600/ 1715 batches | lr 20.00 | ms/batch 40.64 | loss  5.32 | ppl   204.01\n",
            "| epoch   5 |   800/ 1715 batches | lr 20.00 | ms/batch 40.59 | loss  5.28 | ppl   197.17\n",
            "| epoch   5 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.29 | ppl   199.33\n",
            "| epoch   5 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.27 | ppl   193.62\n",
            "| epoch   5 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.26 | ppl   192.52\n",
            "| epoch   5 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.25 | ppl   190.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 77.33s | valid loss  5.68 | valid ppl   292.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1715 batches | lr 20.00 | ms/batch 40.82 | loss  5.25 | ppl   189.98\n",
            "| epoch   6 |   400/ 1715 batches | lr 20.00 | ms/batch 40.62 | loss  5.20 | ppl   181.77\n",
            "| epoch   6 |   600/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.17 | ppl   176.49\n",
            "| epoch   6 |   800/ 1715 batches | lr 20.00 | ms/batch 40.58 | loss  5.14 | ppl   171.30\n",
            "| epoch   6 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.16 | ppl   173.47\n",
            "| epoch   6 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.13 | ppl   169.85\n",
            "| epoch   6 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.53 | loss  5.13 | ppl   168.77\n",
            "| epoch   6 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.13 | ppl   168.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 77.35s | valid loss  5.66 | valid ppl   288.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1715 batches | lr 20.00 | ms/batch 40.79 | loss  5.12 | ppl   168.01\n",
            "| epoch   7 |   400/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.08 | ppl   161.26\n",
            "| epoch   7 |   600/ 1715 batches | lr 20.00 | ms/batch 40.53 | loss  5.06 | ppl   156.83\n",
            "| epoch   7 |   800/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  5.03 | ppl   153.38\n",
            "| epoch   7 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.05 | ppl   155.29\n",
            "| epoch   7 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  5.02 | ppl   151.59\n",
            "| epoch   7 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.57 | loss  5.01 | ppl   150.42\n",
            "| epoch   7 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.62 | loss  5.02 | ppl   152.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 77.35s | valid loss  5.66 | valid ppl   287.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1715 batches | lr 20.00 | ms/batch 40.74 | loss  5.02 | ppl   150.80\n",
            "| epoch   8 |   400/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  4.98 | ppl   145.50\n",
            "| epoch   8 |   600/ 1715 batches | lr 20.00 | ms/batch 40.56 | loss  4.96 | ppl   142.20\n",
            "| epoch   8 |   800/ 1715 batches | lr 20.00 | ms/batch 40.54 | loss  4.94 | ppl   139.14\n",
            "| epoch   8 |  1000/ 1715 batches | lr 20.00 | ms/batch 40.55 | loss  4.95 | ppl   140.57\n",
            "| epoch   8 |  1200/ 1715 batches | lr 20.00 | ms/batch 40.61 | loss  4.92 | ppl   137.56\n",
            "| epoch   8 |  1400/ 1715 batches | lr 20.00 | ms/batch 40.65 | loss  4.92 | ppl   136.99\n",
            "| epoch   8 |  1600/ 1715 batches | lr 20.00 | ms/batch 40.61 | loss  4.93 | ppl   138.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 77.37s | valid loss  5.66 | valid ppl   287.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 5.0\n",
            "val_loss is 5.662391185760498\n",
            "best_val_loss is 5.659702301025391\n",
            "| epoch   9 |   200/ 1715 batches | lr 5.00 | ms/batch 40.73 | loss  4.83 | ppl   125.29\n",
            "| epoch   9 |   400/ 1715 batches | lr 5.00 | ms/batch 40.56 | loss  4.75 | ppl   115.72\n",
            "| epoch   9 |   600/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.69 | ppl   108.96\n",
            "| epoch   9 |   800/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.64 | ppl   103.66\n",
            "| epoch   9 |  1000/ 1715 batches | lr 5.00 | ms/batch 40.62 | loss  4.62 | ppl   101.62\n",
            "| epoch   9 |  1200/ 1715 batches | lr 5.00 | ms/batch 40.63 | loss  4.56 | ppl    95.50\n",
            "| epoch   9 |  1400/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.51 | ppl    91.32\n",
            "| epoch   9 |  1600/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.49 | ppl    89.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 77.36s | valid loss  5.56 | valid ppl   260.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1715 batches | lr 5.00 | ms/batch 40.73 | loss  4.68 | ppl   107.55\n",
            "| epoch  10 |   400/ 1715 batches | lr 5.00 | ms/batch 40.56 | loss  4.62 | ppl   101.86\n",
            "| epoch  10 |   600/ 1715 batches | lr 5.00 | ms/batch 40.55 | loss  4.58 | ppl    97.16\n",
            "| epoch  10 |   800/ 1715 batches | lr 5.00 | ms/batch 40.64 | loss  4.54 | ppl    93.93\n",
            "| epoch  10 |  1000/ 1715 batches | lr 5.00 | ms/batch 40.66 | loss  4.53 | ppl    93.10\n",
            "| epoch  10 |  1200/ 1715 batches | lr 5.00 | ms/batch 40.55 | loss  4.49 | ppl    89.45\n",
            "| epoch  10 |  1400/ 1715 batches | lr 5.00 | ms/batch 40.55 | loss  4.46 | ppl    86.50\n",
            "| epoch  10 |  1600/ 1715 batches | lr 5.00 | ms/batch 40.54 | loss  4.46 | ppl    86.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 77.36s | valid loss  5.56 | valid ppl   259.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1715 batches | lr 5.00 | ms/batch 40.77 | loss  4.61 | ppl   100.30\n",
            "| epoch  11 |   400/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.56 | ppl    95.42\n",
            "| epoch  11 |   600/ 1715 batches | lr 5.00 | ms/batch 40.62 | loss  4.52 | ppl    91.91\n",
            "| epoch  11 |   800/ 1715 batches | lr 5.00 | ms/batch 40.62 | loss  4.49 | ppl    88.92\n",
            "| epoch  11 |  1000/ 1715 batches | lr 5.00 | ms/batch 40.55 | loss  4.48 | ppl    88.28\n",
            "| epoch  11 |  1200/ 1715 batches | lr 5.00 | ms/batch 40.57 | loss  4.45 | ppl    85.24\n",
            "| epoch  11 |  1400/ 1715 batches | lr 5.00 | ms/batch 40.56 | loss  4.41 | ppl    82.62\n",
            "| epoch  11 |  1600/ 1715 batches | lr 5.00 | ms/batch 40.54 | loss  4.42 | ppl    83.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 77.36s | valid loss  5.56 | valid ppl   259.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 1.25\n",
            "val_loss is 5.560654163360596\n",
            "best_val_loss is 5.559659004211426\n",
            "| epoch  12 |   200/ 1715 batches | lr 1.25 | ms/batch 40.79 | loss  4.55 | ppl    94.61\n",
            "| epoch  12 |   400/ 1715 batches | lr 1.25 | ms/batch 40.67 | loss  4.50 | ppl    89.61\n",
            "| epoch  12 |   600/ 1715 batches | lr 1.25 | ms/batch 40.59 | loss  4.45 | ppl    85.53\n",
            "| epoch  12 |   800/ 1715 batches | lr 1.25 | ms/batch 40.56 | loss  4.42 | ppl    82.72\n",
            "| epoch  12 |  1000/ 1715 batches | lr 1.25 | ms/batch 40.55 | loss  4.39 | ppl    80.90\n",
            "| epoch  12 |  1200/ 1715 batches | lr 1.25 | ms/batch 40.55 | loss  4.34 | ppl    76.97\n",
            "| epoch  12 |  1400/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.30 | ppl    73.60\n",
            "| epoch  12 |  1600/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.29 | ppl    72.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 77.39s | valid loss  5.54 | valid ppl   253.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1715 batches | lr 1.25 | ms/batch 40.84 | loss  4.51 | ppl    90.84\n",
            "| epoch  13 |   400/ 1715 batches | lr 1.25 | ms/batch 40.55 | loss  4.46 | ppl    86.73\n",
            "| epoch  13 |   600/ 1715 batches | lr 1.25 | ms/batch 40.58 | loss  4.41 | ppl    82.50\n",
            "| epoch  13 |   800/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.38 | ppl    80.22\n",
            "| epoch  13 |  1000/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.37 | ppl    78.72\n",
            "| epoch  13 |  1200/ 1715 batches | lr 1.25 | ms/batch 40.54 | loss  4.32 | ppl    75.18\n",
            "| epoch  13 |  1400/ 1715 batches | lr 1.25 | ms/batch 40.55 | loss  4.28 | ppl    72.55\n",
            "| epoch  13 |  1600/ 1715 batches | lr 1.25 | ms/batch 40.61 | loss  4.28 | ppl    72.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 77.38s | valid loss  5.54 | valid ppl   253.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1715 batches | lr 1.25 | ms/batch 40.75 | loss  4.49 | ppl    89.13\n",
            "| epoch  14 |   400/ 1715 batches | lr 1.25 | ms/batch 40.54 | loss  4.44 | ppl    84.78\n",
            "| epoch  14 |   600/ 1715 batches | lr 1.25 | ms/batch 40.54 | loss  4.39 | ppl    80.98\n",
            "| epoch  14 |   800/ 1715 batches | lr 1.25 | ms/batch 40.56 | loss  4.36 | ppl    78.53\n",
            "| epoch  14 |  1000/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.35 | ppl    77.56\n",
            "| epoch  14 |  1200/ 1715 batches | lr 1.25 | ms/batch 40.57 | loss  4.31 | ppl    74.46\n",
            "| epoch  14 |  1400/ 1715 batches | lr 1.25 | ms/batch 40.63 | loss  4.28 | ppl    71.89\n",
            "| epoch  14 |  1600/ 1715 batches | lr 1.25 | ms/batch 40.64 | loss  4.28 | ppl    71.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 77.36s | valid loss  5.54 | valid ppl   253.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.3125\n",
            "val_loss is 5.537214756011963\n",
            "best_val_loss is 5.536043167114258\n",
            "| epoch  15 |   200/ 1715 batches | lr 0.31 | ms/batch 40.76 | loss  4.48 | ppl    87.98\n",
            "| epoch  15 |   400/ 1715 batches | lr 0.31 | ms/batch 40.56 | loss  4.42 | ppl    83.48\n",
            "| epoch  15 |   600/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.38 | ppl    79.66\n",
            "| epoch  15 |   800/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.34 | ppl    76.83\n",
            "| epoch  15 |  1000/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.33 | ppl    75.58\n",
            "| epoch  15 |  1200/ 1715 batches | lr 0.31 | ms/batch 40.66 | loss  4.28 | ppl    72.19\n",
            "| epoch  15 |  1400/ 1715 batches | lr 0.31 | ms/batch 40.64 | loss  4.24 | ppl    69.30\n",
            "| epoch  15 |  1600/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.24 | ppl    69.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 77.36s | valid loss  5.53 | valid ppl   252.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1715 batches | lr 0.31 | ms/batch 40.81 | loss  4.46 | ppl    86.77\n",
            "| epoch  16 |   400/ 1715 batches | lr 0.31 | ms/batch 40.57 | loss  4.41 | ppl    82.52\n",
            "| epoch  16 |   600/ 1715 batches | lr 0.31 | ms/batch 40.57 | loss  4.37 | ppl    78.77\n",
            "| epoch  16 |   800/ 1715 batches | lr 0.31 | ms/batch 40.59 | loss  4.33 | ppl    76.30\n",
            "| epoch  16 |  1000/ 1715 batches | lr 0.31 | ms/batch 40.63 | loss  4.32 | ppl    75.41\n",
            "| epoch  16 |  1200/ 1715 batches | lr 0.31 | ms/batch 40.61 | loss  4.28 | ppl    72.07\n",
            "| epoch  16 |  1400/ 1715 batches | lr 0.31 | ms/batch 40.56 | loss  4.24 | ppl    69.20\n",
            "| epoch  16 |  1600/ 1715 batches | lr 0.31 | ms/batch 40.59 | loss  4.23 | ppl    68.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 77.38s | valid loss  5.53 | valid ppl   251.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1715 batches | lr 0.31 | ms/batch 40.70 | loss  4.46 | ppl    86.11\n",
            "| epoch  17 |   400/ 1715 batches | lr 0.31 | ms/batch 40.53 | loss  4.41 | ppl    82.30\n",
            "| epoch  17 |   600/ 1715 batches | lr 0.31 | ms/batch 40.62 | loss  4.36 | ppl    78.44\n",
            "| epoch  17 |   800/ 1715 batches | lr 0.31 | ms/batch 40.67 | loss  4.33 | ppl    76.30\n",
            "| epoch  17 |  1000/ 1715 batches | lr 0.31 | ms/batch 40.58 | loss  4.32 | ppl    74.91\n",
            "| epoch  17 |  1200/ 1715 batches | lr 0.31 | ms/batch 40.56 | loss  4.27 | ppl    71.47\n",
            "| epoch  17 |  1400/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.24 | ppl    69.07\n",
            "| epoch  17 |  1600/ 1715 batches | lr 0.31 | ms/batch 40.55 | loss  4.23 | ppl    69.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 77.36s | valid loss  5.53 | valid ppl   251.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1715 batches | lr 0.31 | ms/batch 40.79 | loss  4.45 | ppl    85.70\n",
            "| epoch  18 |   400/ 1715 batches | lr 0.31 | ms/batch 40.63 | loss  4.40 | ppl    81.65\n",
            "| epoch  18 |   600/ 1715 batches | lr 0.31 | ms/batch 40.61 | loss  4.35 | ppl    77.86\n",
            "| epoch  18 |   800/ 1715 batches | lr 0.31 | ms/batch 40.54 | loss  4.33 | ppl    75.63\n",
            "| epoch  18 |  1000/ 1715 batches | lr 0.31 | ms/batch 40.58 | loss  4.31 | ppl    74.51\n",
            "| epoch  18 |  1200/ 1715 batches | lr 0.31 | ms/batch 40.56 | loss  4.27 | ppl    71.54\n",
            "| epoch  18 |  1400/ 1715 batches | lr 0.31 | ms/batch 40.57 | loss  4.23 | ppl    68.97\n",
            "| epoch  18 |  1600/ 1715 batches | lr 0.31 | ms/batch 40.54 | loss  4.23 | ppl    68.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 77.34s | valid loss  5.53 | valid ppl   251.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.078125\n",
            "val_loss is 5.529358386993408\n",
            "best_val_loss is 5.528998851776123\n",
            "| epoch  19 |   200/ 1715 batches | lr 0.08 | ms/batch 40.91 | loss  4.45 | ppl    85.36\n",
            "| epoch  19 |   400/ 1715 batches | lr 0.08 | ms/batch 40.61 | loss  4.40 | ppl    81.32\n",
            "| epoch  19 |   600/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.35 | ppl    77.70\n",
            "| epoch  19 |   800/ 1715 batches | lr 0.08 | ms/batch 40.54 | loss  4.32 | ppl    75.15\n",
            "| epoch  19 |  1000/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.31 | ppl    74.17\n",
            "| epoch  19 |  1200/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.26 | ppl    70.76\n",
            "| epoch  19 |  1400/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.22 | ppl    68.23\n",
            "| epoch  19 |  1600/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.22 | ppl    67.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 77.35s | valid loss  5.53 | valid ppl   251.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1715 batches | lr 0.08 | ms/batch 40.83 | loss  4.45 | ppl    85.34\n",
            "| epoch  20 |   400/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.40 | ppl    81.31\n",
            "| epoch  20 |   600/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.35 | ppl    77.41\n",
            "| epoch  20 |   800/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.32 | ppl    74.83\n",
            "| epoch  20 |  1000/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.30 | ppl    73.90\n",
            "| epoch  20 |  1200/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.26 | ppl    70.65\n",
            "| epoch  20 |  1400/ 1715 batches | lr 0.08 | ms/batch 40.58 | loss  4.22 | ppl    68.00\n",
            "| epoch  20 |  1600/ 1715 batches | lr 0.08 | ms/batch 40.64 | loss  4.22 | ppl    67.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 77.34s | valid loss  5.53 | valid ppl   250.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1715 batches | lr 0.08 | ms/batch 40.79 | loss  4.44 | ppl    85.10\n",
            "| epoch  21 |   400/ 1715 batches | lr 0.08 | ms/batch 40.54 | loss  4.40 | ppl    81.06\n",
            "| epoch  21 |   600/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.35 | ppl    77.15\n",
            "| epoch  21 |   800/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.31 | ppl    74.72\n",
            "| epoch  21 |  1000/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.30 | ppl    73.67\n",
            "| epoch  21 |  1200/ 1715 batches | lr 0.08 | ms/batch 40.61 | loss  4.26 | ppl    70.57\n",
            "| epoch  21 |  1400/ 1715 batches | lr 0.08 | ms/batch 40.62 | loss  4.22 | ppl    68.36\n",
            "| epoch  21 |  1600/ 1715 batches | lr 0.08 | ms/batch 40.60 | loss  4.22 | ppl    68.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 77.34s | valid loss  5.52 | valid ppl   250.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1715 batches | lr 0.08 | ms/batch 40.76 | loss  4.44 | ppl    84.90\n",
            "| epoch  22 |   400/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.39 | ppl    81.01\n",
            "| epoch  22 |   600/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.34 | ppl    76.87\n",
            "| epoch  22 |   800/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.31 | ppl    74.63\n",
            "| epoch  22 |  1000/ 1715 batches | lr 0.08 | ms/batch 40.65 | loss  4.30 | ppl    73.98\n",
            "| epoch  22 |  1200/ 1715 batches | lr 0.08 | ms/batch 40.65 | loss  4.26 | ppl    70.53\n",
            "| epoch  22 |  1400/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.22 | ppl    68.06\n",
            "| epoch  22 |  1600/ 1715 batches | lr 0.08 | ms/batch 40.54 | loss  4.22 | ppl    68.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 77.34s | valid loss  5.52 | valid ppl   250.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1715 batches | lr 0.08 | ms/batch 40.71 | loss  4.44 | ppl    84.85\n",
            "| epoch  23 |   400/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.39 | ppl    80.60\n",
            "| epoch  23 |   600/ 1715 batches | lr 0.08 | ms/batch 40.56 | loss  4.35 | ppl    77.39\n",
            "| epoch  23 |   800/ 1715 batches | lr 0.08 | ms/batch 40.64 | loss  4.31 | ppl    74.64\n",
            "| epoch  23 |  1000/ 1715 batches | lr 0.08 | ms/batch 40.64 | loss  4.30 | ppl    73.84\n",
            "| epoch  23 |  1200/ 1715 batches | lr 0.08 | ms/batch 40.53 | loss  4.26 | ppl    70.59\n",
            "| epoch  23 |  1400/ 1715 batches | lr 0.08 | ms/batch 40.57 | loss  4.22 | ppl    68.09\n",
            "| epoch  23 |  1600/ 1715 batches | lr 0.08 | ms/batch 40.55 | loss  4.22 | ppl    68.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 77.34s | valid loss  5.52 | valid ppl   250.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0390625\n",
            "val_loss is 5.524685382843018\n",
            "best_val_loss is 5.524491310119629\n",
            "| epoch  24 |   200/ 1715 batches | lr 0.04 | ms/batch 40.81 | loss  4.44 | ppl    84.48\n",
            "| epoch  24 |   400/ 1715 batches | lr 0.04 | ms/batch 40.59 | loss  4.39 | ppl    80.68\n",
            "| epoch  24 |   600/ 1715 batches | lr 0.04 | ms/batch 40.66 | loss  4.34 | ppl    77.04\n",
            "| epoch  24 |   800/ 1715 batches | lr 0.04 | ms/batch 40.61 | loss  4.31 | ppl    74.56\n",
            "| epoch  24 |  1000/ 1715 batches | lr 0.04 | ms/batch 40.57 | loss  4.30 | ppl    73.56\n",
            "| epoch  24 |  1200/ 1715 batches | lr 0.04 | ms/batch 40.54 | loss  4.25 | ppl    70.36\n",
            "| epoch  24 |  1400/ 1715 batches | lr 0.04 | ms/batch 40.55 | loss  4.22 | ppl    68.09\n",
            "| epoch  24 |  1600/ 1715 batches | lr 0.04 | ms/batch 40.57 | loss  4.22 | ppl    67.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 77.37s | valid loss  5.52 | valid ppl   250.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1715 batches | lr 0.04 | ms/batch 40.76 | loss  4.44 | ppl    84.74\n",
            "| epoch  25 |   400/ 1715 batches | lr 0.04 | ms/batch 40.64 | loss  4.39 | ppl    80.58\n",
            "| epoch  25 |   600/ 1715 batches | lr 0.04 | ms/batch 40.57 | loss  4.34 | ppl    76.88\n",
            "| epoch  25 |   800/ 1715 batches | lr 0.04 | ms/batch 40.56 | loss  4.31 | ppl    74.62\n",
            "| epoch  25 |  1000/ 1715 batches | lr 0.04 | ms/batch 40.57 | loss  4.30 | ppl    73.74\n",
            "| epoch  25 |  1200/ 1715 batches | lr 0.04 | ms/batch 40.55 | loss  4.25 | ppl    70.33\n",
            "| epoch  25 |  1400/ 1715 batches | lr 0.04 | ms/batch 40.54 | loss  4.22 | ppl    67.87\n",
            "| epoch  25 |  1600/ 1715 batches | lr 0.04 | ms/batch 40.54 | loss  4.22 | ppl    68.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 77.33s | valid loss  5.52 | valid ppl   250.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1715 batches | lr 0.04 | ms/batch 40.80 | loss  4.44 | ppl    84.57\n",
            "| epoch  26 |   400/ 1715 batches | lr 0.04 | ms/batch 40.55 | loss  4.39 | ppl    80.60\n",
            "| epoch  26 |   600/ 1715 batches | lr 0.04 | ms/batch 40.54 | loss  4.34 | ppl    76.84\n",
            "| epoch  26 |   800/ 1715 batches | lr 0.04 | ms/batch 40.53 | loss  4.31 | ppl    74.51\n",
            "| epoch  26 |  1000/ 1715 batches | lr 0.04 | ms/batch 40.57 | loss  4.30 | ppl    73.37\n",
            "| epoch  26 |  1200/ 1715 batches | lr 0.04 | ms/batch 40.58 | loss  4.25 | ppl    70.23\n",
            "| epoch  26 |  1400/ 1715 batches | lr 0.04 | ms/batch 40.56 | loss  4.22 | ppl    68.06\n",
            "| epoch  26 |  1600/ 1715 batches | lr 0.04 | ms/batch 40.61 | loss  4.22 | ppl    67.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 77.35s | valid loss  5.52 | valid ppl   250.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.01953125\n",
            "val_loss is 5.5237579345703125\n",
            "best_val_loss is 5.523637771606445\n",
            "| epoch  27 |   200/ 1715 batches | lr 0.02 | ms/batch 40.82 | loss  4.44 | ppl    84.75\n",
            "| epoch  27 |   400/ 1715 batches | lr 0.02 | ms/batch 40.57 | loss  4.39 | ppl    80.53\n",
            "| epoch  27 |   600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.34 | ppl    76.76\n",
            "| epoch  27 |   800/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.31 | ppl    74.24\n",
            "| epoch  27 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.30 | ppl    73.37\n",
            "| epoch  27 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.57 | loss  4.25 | ppl    70.04\n",
            "| epoch  27 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.63 | loss  4.22 | ppl    67.81\n",
            "| epoch  27 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.66 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 77.37s | valid loss  5.52 | valid ppl   250.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1715 batches | lr 0.02 | ms/batch 40.74 | loss  4.43 | ppl    84.32\n",
            "| epoch  28 |   400/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.39 | ppl    80.40\n",
            "| epoch  28 |   600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.34 | ppl    76.84\n",
            "| epoch  28 |   800/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.31 | ppl    74.61\n",
            "| epoch  28 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.58 | loss  4.30 | ppl    73.60\n",
            "| epoch  28 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.64 | loss  4.25 | ppl    70.29\n",
            "| epoch  28 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.64 | loss  4.22 | ppl    67.95\n",
            "| epoch  28 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.22 | ppl    67.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 77.34s | valid loss  5.52 | valid ppl   250.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1715 batches | lr 0.02 | ms/batch 40.79 | loss  4.43 | ppl    84.11\n",
            "| epoch  29 |   400/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.39 | ppl    80.37\n",
            "| epoch  29 |   600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.34 | ppl    76.75\n",
            "| epoch  29 |   800/ 1715 batches | lr 0.02 | ms/batch 40.59 | loss  4.31 | ppl    74.32\n",
            "| epoch  29 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.65 | loss  4.30 | ppl    73.48\n",
            "| epoch  29 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.60 | loss  4.25 | ppl    70.45\n",
            "| epoch  29 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.21 | ppl    67.67\n",
            "| epoch  29 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.21 | ppl    67.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 77.32s | valid loss  5.52 | valid ppl   250.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1715 batches | lr 0.02 | ms/batch 40.80 | loss  4.44 | ppl    84.50\n",
            "| epoch  30 |   400/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.39 | ppl    80.43\n",
            "| epoch  30 |   600/ 1715 batches | lr 0.02 | ms/batch 40.62 | loss  4.34 | ppl    76.55\n",
            "| epoch  30 |   800/ 1715 batches | lr 0.02 | ms/batch 40.63 | loss  4.31 | ppl    74.23\n",
            "| epoch  30 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.57 | loss  4.29 | ppl    73.29\n",
            "| epoch  30 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.57 | loss  4.25 | ppl    70.43\n",
            "| epoch  30 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.22 | ppl    67.80\n",
            "| epoch  30 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.22 | ppl    68.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 77.34s | valid loss  5.52 | valid ppl   250.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1715 batches | lr 0.02 | ms/batch 40.78 | loss  4.44 | ppl    84.47\n",
            "| epoch  31 |   400/ 1715 batches | lr 0.02 | ms/batch 40.65 | loss  4.38 | ppl    80.14\n",
            "| epoch  31 |   600/ 1715 batches | lr 0.02 | ms/batch 40.66 | loss  4.34 | ppl    76.85\n",
            "| epoch  31 |   800/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.31 | ppl    74.41\n",
            "| epoch  31 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.30 | ppl    73.48\n",
            "| epoch  31 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.25 | ppl    70.10\n",
            "| epoch  31 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.22 | ppl    67.92\n",
            "| epoch  31 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.22 | ppl    67.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 77.33s | valid loss  5.52 | valid ppl   250.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1715 batches | lr 0.02 | ms/batch 40.84 | loss  4.43 | ppl    84.21\n",
            "| epoch  32 |   400/ 1715 batches | lr 0.02 | ms/batch 40.60 | loss  4.39 | ppl    80.61\n",
            "| epoch  32 |   600/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.34 | ppl    76.65\n",
            "| epoch  32 |   800/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.31 | ppl    74.23\n",
            "| epoch  32 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.30 | ppl    73.37\n",
            "| epoch  32 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.25 | ppl    70.20\n",
            "| epoch  32 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.22 | ppl    67.88\n",
            "| epoch  32 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.57 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 77.33s | valid loss  5.52 | valid ppl   250.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1715 batches | lr 0.02 | ms/batch 40.73 | loss  4.43 | ppl    84.16\n",
            "| epoch  33 |   400/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.39 | ppl    80.56\n",
            "| epoch  33 |   600/ 1715 batches | lr 0.02 | ms/batch 40.54 | loss  4.34 | ppl    76.53\n",
            "| epoch  33 |   800/ 1715 batches | lr 0.02 | ms/batch 40.55 | loss  4.31 | ppl    74.37\n",
            "| epoch  33 |  1000/ 1715 batches | lr 0.02 | ms/batch 40.58 | loss  4.29 | ppl    73.32\n",
            "| epoch  33 |  1200/ 1715 batches | lr 0.02 | ms/batch 40.56 | loss  4.25 | ppl    70.34\n",
            "| epoch  33 |  1400/ 1715 batches | lr 0.02 | ms/batch 40.60 | loss  4.22 | ppl    67.72\n",
            "| epoch  33 |  1600/ 1715 batches | lr 0.02 | ms/batch 40.64 | loss  4.22 | ppl    67.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 77.32s | valid loss  5.52 | valid ppl   250.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.009765625\n",
            "val_loss is 5.522876262664795\n",
            "best_val_loss is 5.522805690765381\n",
            "| epoch  34 |   200/ 1715 batches | lr 0.01 | ms/batch 40.74 | loss  4.44 | ppl    84.35\n",
            "| epoch  34 |   400/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.38 | ppl    80.10\n",
            "| epoch  34 |   600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.34 | ppl    76.75\n",
            "| epoch  34 |   800/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.31 | ppl    74.24\n",
            "| epoch  34 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.29 | ppl    73.28\n",
            "| epoch  34 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.62 | loss  4.25 | ppl    70.20\n",
            "| epoch  34 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.64 | loss  4.22 | ppl    67.71\n",
            "| epoch  34 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.22 | ppl    67.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 77.34s | valid loss  5.52 | valid ppl   250.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1715 batches | lr 0.01 | ms/batch 40.70 | loss  4.43 | ppl    84.15\n",
            "| epoch  35 |   400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.39 | ppl    80.31\n",
            "| epoch  35 |   600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.34 | ppl    76.65\n",
            "| epoch  35 |   800/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.31 | ppl    74.25\n",
            "| epoch  35 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.62 | loss  4.30 | ppl    73.43\n",
            "| epoch  35 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.64 | loss  4.25 | ppl    70.13\n",
            "| epoch  35 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.82\n",
            "| epoch  35 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 77.34s | valid loss  5.52 | valid ppl   250.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1715 batches | lr 0.01 | ms/batch 40.79 | loss  4.43 | ppl    84.07\n",
            "| epoch  36 |   400/ 1715 batches | lr 0.01 | ms/batch 40.53 | loss  4.39 | ppl    80.24\n",
            "| epoch  36 |   600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.34 | ppl    76.65\n",
            "| epoch  36 |   800/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.31 | ppl    74.32\n",
            "| epoch  36 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.29 | ppl    73.24\n",
            "| epoch  36 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.25 | ppl    70.26\n",
            "| epoch  36 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.99\n",
            "| epoch  36 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.22 | ppl    67.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 77.35s | valid loss  5.52 | valid ppl   250.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00732421875\n",
            "val_loss is 5.522685527801514\n",
            "best_val_loss is 5.522683620452881\n",
            "| epoch  37 |   200/ 1715 batches | lr 0.01 | ms/batch 40.81 | loss  4.43 | ppl    83.92\n",
            "| epoch  37 |   400/ 1715 batches | lr 0.01 | ms/batch 40.60 | loss  4.38 | ppl    80.20\n",
            "| epoch  37 |   600/ 1715 batches | lr 0.01 | ms/batch 40.64 | loss  4.34 | ppl    76.47\n",
            "| epoch  37 |   800/ 1715 batches | lr 0.01 | ms/batch 40.61 | loss  4.31 | ppl    74.21\n",
            "| epoch  37 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.30 | ppl    73.48\n",
            "| epoch  37 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.25 | ppl    70.31\n",
            "| epoch  37 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.22 | ppl    67.80\n",
            "| epoch  37 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.21 | ppl    67.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 77.38s | valid loss  5.52 | valid ppl   250.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1715 batches | lr 0.01 | ms/batch 40.85 | loss  4.44 | ppl    84.44\n",
            "| epoch  38 |   400/ 1715 batches | lr 0.01 | ms/batch 40.66 | loss  4.38 | ppl    80.18\n",
            "| epoch  38 |   600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.34 | ppl    76.47\n",
            "| epoch  38 |   800/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.31 | ppl    74.36\n",
            "| epoch  38 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.29 | ppl    73.20\n",
            "| epoch  38 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.25 | ppl    70.16\n",
            "| epoch  38 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.71\n",
            "| epoch  38 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 77.37s | valid loss  5.52 | valid ppl   250.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1715 batches | lr 0.01 | ms/batch 40.88 | loss  4.43 | ppl    84.23\n",
            "| epoch  39 |   400/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.39 | ppl    80.25\n",
            "| epoch  39 |   600/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.34 | ppl    76.70\n",
            "| epoch  39 |   800/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.31 | ppl    74.13\n",
            "| epoch  39 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.30 | ppl    73.49\n",
            "| epoch  39 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.25 | ppl    70.13\n",
            "| epoch  39 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.22 | ppl    67.83\n",
            "| epoch  39 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.61 | loss  4.21 | ppl    67.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1715 batches | lr 0.01 | ms/batch 40.82 | loss  4.43 | ppl    84.10\n",
            "| epoch  40 |   400/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.38 | ppl    80.07\n",
            "| epoch  40 |   600/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.34 | ppl    76.42\n",
            "| epoch  40 |   800/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.31 | ppl    74.12\n",
            "| epoch  40 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.30 | ppl    73.56\n",
            "| epoch  40 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.25 | ppl    70.10\n",
            "| epoch  40 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.63 | loss  4.21 | ppl    67.69\n",
            "| epoch  40 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.63 | loss  4.22 | ppl    67.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 77.35s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 1715 batches | lr 0.01 | ms/batch 40.82 | loss  4.43 | ppl    84.07\n",
            "| epoch  41 |   400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.38 | ppl    80.12\n",
            "| epoch  41 |   600/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.34 | ppl    76.70\n",
            "| epoch  41 |   800/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.31 | ppl    74.40\n",
            "| epoch  41 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.29 | ppl    73.15\n",
            "| epoch  41 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.66 | loss  4.25 | ppl    69.96\n",
            "| epoch  41 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.61 | loss  4.22 | ppl    67.76\n",
            "| epoch  41 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.21 | ppl    67.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 77.35s | valid loss  5.52 | valid ppl   250.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0054931640625\n",
            "val_loss is 5.522606372833252\n",
            "best_val_loss is 5.522584438323975\n",
            "| epoch  42 |   200/ 1715 batches | lr 0.01 | ms/batch 40.75 | loss  4.43 | ppl    84.01\n",
            "| epoch  42 |   400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.38 | ppl    80.22\n",
            "| epoch  42 |   600/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.34 | ppl    76.49\n",
            "| epoch  42 |   800/ 1715 batches | lr 0.01 | ms/batch 40.59 | loss  4.31 | ppl    74.07\n",
            "| epoch  42 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.29 | ppl    73.32\n",
            "| epoch  42 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.59 | loss  4.25 | ppl    70.24\n",
            "| epoch  42 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.21 | ppl    67.62\n",
            "| epoch  42 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.21 | ppl    67.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 77.36s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 1715 batches | lr 0.01 | ms/batch 40.76 | loss  4.43 | ppl    84.05\n",
            "| epoch  43 |   400/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.38 | ppl    80.05\n",
            "| epoch  43 |   600/ 1715 batches | lr 0.01 | ms/batch 40.63 | loss  4.34 | ppl    76.61\n",
            "| epoch  43 |   800/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.31 | ppl    74.39\n",
            "| epoch  43 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.30 | ppl    73.39\n",
            "| epoch  43 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.25 | ppl    70.22\n",
            "| epoch  43 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.22 | ppl    67.76\n",
            "| epoch  43 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.21 | ppl    67.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 77.35s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 1715 batches | lr 0.01 | ms/batch 40.81 | loss  4.43 | ppl    83.94\n",
            "| epoch  44 |   400/ 1715 batches | lr 0.01 | ms/batch 40.64 | loss  4.38 | ppl    80.08\n",
            "| epoch  44 |   600/ 1715 batches | lr 0.01 | ms/batch 40.63 | loss  4.34 | ppl    76.43\n",
            "| epoch  44 |   800/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.31 | ppl    74.15\n",
            "| epoch  44 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.30 | ppl    73.39\n",
            "| epoch  44 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.55 | loss  4.25 | ppl    70.05\n",
            "| epoch  44 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.22 | ppl    67.79\n",
            "| epoch  44 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.21 | ppl    67.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 77.34s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 1715 batches | lr 0.01 | ms/batch 40.89 | loss  4.43 | ppl    84.28\n",
            "| epoch  45 |   400/ 1715 batches | lr 0.01 | ms/batch 40.61 | loss  4.38 | ppl    80.07\n",
            "| epoch  45 |   600/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.34 | ppl    76.34\n",
            "| epoch  45 |   800/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.31 | ppl    74.08\n",
            "| epoch  45 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.30 | ppl    73.40\n",
            "| epoch  45 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.25 | ppl    69.90\n",
            "| epoch  45 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.22 | ppl    67.75\n",
            "| epoch  45 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.22 | ppl    67.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/ 1715 batches | lr 0.01 | ms/batch 40.81 | loss  4.43 | ppl    84.10\n",
            "| epoch  46 |   400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.38 | ppl    80.05\n",
            "| epoch  46 |   600/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.34 | ppl    76.77\n",
            "| epoch  46 |   800/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.31 | ppl    74.29\n",
            "| epoch  46 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.30 | ppl    73.45\n",
            "| epoch  46 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.54 | loss  4.25 | ppl    70.04\n",
            "| epoch  46 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.22 | ppl    67.70\n",
            "| epoch  46 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.21 | ppl    67.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 77.35s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/ 1715 batches | lr 0.01 | ms/batch 40.81 | loss  4.43 | ppl    84.27\n",
            "| epoch  47 |   400/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.38 | ppl    80.11\n",
            "| epoch  47 |   600/ 1715 batches | lr 0.01 | ms/batch 40.56 | loss  4.34 | ppl    76.58\n",
            "| epoch  47 |   800/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.31 | ppl    74.19\n",
            "| epoch  47 |  1000/ 1715 batches | lr 0.01 | ms/batch 40.57 | loss  4.29 | ppl    73.22\n",
            "| epoch  47 |  1200/ 1715 batches | lr 0.01 | ms/batch 40.65 | loss  4.25 | ppl    70.18\n",
            "| epoch  47 |  1400/ 1715 batches | lr 0.01 | ms/batch 40.62 | loss  4.22 | ppl    67.91\n",
            "| epoch  47 |  1600/ 1715 batches | lr 0.01 | ms/batch 40.58 | loss  4.21 | ppl    67.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.004119873046875\n",
            "val_loss is 5.522550582885742\n",
            "best_val_loss is 5.522518157958984\n",
            "| epoch  48 |   200/ 1715 batches | lr 0.00 | ms/batch 40.76 | loss  4.43 | ppl    84.28\n",
            "| epoch  48 |   400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.38 | ppl    79.91\n",
            "| epoch  48 |   600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.34 | ppl    76.43\n",
            "| epoch  48 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.25\n",
            "| epoch  48 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.30 | ppl    73.44\n",
            "| epoch  48 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.25 | ppl    70.00\n",
            "| epoch  48 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.80\n",
            "| epoch  48 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.22 | ppl    67.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00308990478515625\n",
            "val_loss is 5.522521018981934\n",
            "best_val_loss is 5.522518157958984\n",
            "| epoch  49 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.43 | ppl    84.22\n",
            "| epoch  49 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    79.82\n",
            "| epoch  49 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.55\n",
            "| epoch  49 |   800/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.31 | ppl    74.17\n",
            "| epoch  49 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.29 | ppl    73.24\n",
            "| epoch  49 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    69.96\n",
            "| epoch  49 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.81\n",
            "| epoch  49 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 77.35s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/ 1715 batches | lr 0.00 | ms/batch 40.76 | loss  4.43 | ppl    83.87\n",
            "| epoch  50 |   400/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.38 | ppl    80.18\n",
            "| epoch  50 |   600/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.34 | ppl    76.52\n",
            "| epoch  50 |   800/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.30 | ppl    73.94\n",
            "| epoch  50 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.29 | ppl    73.20\n",
            "| epoch  50 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    70.05\n",
            "| epoch  50 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.54\n",
            "| epoch  50 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 77.33s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   200/ 1715 batches | lr 0.00 | ms/batch 40.77 | loss  4.43 | ppl    83.98\n",
            "| epoch  51 |   400/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.39 | ppl    80.27\n",
            "| epoch  51 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.47\n",
            "| epoch  51 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    74.00\n",
            "| epoch  51 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.11\n",
            "| epoch  51 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.41\n",
            "| epoch  51 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.73\n",
            "| epoch  51 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 77.34s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    84.11\n",
            "| epoch  52 |   400/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.39 | ppl    80.32\n",
            "| epoch  52 |   600/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.34 | ppl    76.38\n",
            "| epoch  52 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.20\n",
            "| epoch  52 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.16\n",
            "| epoch  52 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.08\n",
            "| epoch  52 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.69\n",
            "| epoch  52 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.21 | ppl    67.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 77.35s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0023174285888671875\n",
            "val_loss is 5.5224995613098145\n",
            "best_val_loss is 5.522491931915283\n",
            "| epoch  53 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.42 | ppl    83.49\n",
            "| epoch  53 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.10\n",
            "| epoch  53 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.33\n",
            "| epoch  53 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.21\n",
            "| epoch  53 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.29 | ppl    73.10\n",
            "| epoch  53 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.25\n",
            "| epoch  53 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.22 | ppl    67.72\n",
            "| epoch  53 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.22 | ppl    67.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 77.35s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   200/ 1715 batches | lr 0.00 | ms/batch 40.71 | loss  4.43 | ppl    84.10\n",
            "| epoch  54 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    79.95\n",
            "| epoch  54 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.50\n",
            "| epoch  54 |   800/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.30 | ppl    73.99\n",
            "| epoch  54 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.29 | ppl    73.17\n",
            "| epoch  54 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.25 | ppl    70.05\n",
            "| epoch  54 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.21 | ppl    67.60\n",
            "| epoch  54 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 77.36s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0017380714416503906\n",
            "val_loss is 5.522497653961182\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  55 |   200/ 1715 batches | lr 0.00 | ms/batch 40.84 | loss  4.43 | ppl    84.17\n",
            "| epoch  55 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.39 | ppl    80.29\n",
            "| epoch  55 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.61\n",
            "| epoch  55 |   800/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.31 | ppl    74.30\n",
            "| epoch  55 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.29 | ppl    73.05\n",
            "| epoch  55 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.25 | ppl    70.22\n",
            "| epoch  55 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.56\n",
            "| epoch  55 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 77.37s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.001303553581237793\n",
            "val_loss is 5.5224995613098145\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  56 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    84.00\n",
            "| epoch  56 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.39 | ppl    80.32\n",
            "| epoch  56 |   600/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.34 | ppl    76.74\n",
            "| epoch  56 |   800/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.31 | ppl    74.12\n",
            "| epoch  56 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    73.47\n",
            "| epoch  56 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.10\n",
            "| epoch  56 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.80\n",
            "| epoch  56 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 77.38s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0009776651859283447\n",
            "val_loss is 5.522492408752441\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  57 |   200/ 1715 batches | lr 0.00 | ms/batch 40.76 | loss  4.44 | ppl    84.38\n",
            "| epoch  57 |   400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.39 | ppl    80.27\n",
            "| epoch  57 |   600/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.34 | ppl    76.36\n",
            "| epoch  57 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.30 | ppl    74.05\n",
            "| epoch  57 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.29 | ppl    73.15\n",
            "| epoch  57 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.13\n",
            "| epoch  57 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.67\n",
            "| epoch  57 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 77.37s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0009287819266319275\n",
            "val_loss is 5.522498607635498\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  58 |   200/ 1715 batches | lr 0.00 | ms/batch 40.85 | loss  4.43 | ppl    84.22\n",
            "| epoch  58 |   400/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.38 | ppl    80.17\n",
            "| epoch  58 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.50\n",
            "| epoch  58 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.16\n",
            "| epoch  58 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.07\n",
            "| epoch  58 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    69.84\n",
            "| epoch  58 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.71\n",
            "| epoch  58 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 77.36s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.000882342830300331\n",
            "val_loss is 5.522499084472656\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  59 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    84.06\n",
            "| epoch  59 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.05\n",
            "| epoch  59 |   600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.34 | ppl    76.50\n",
            "| epoch  59 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.28\n",
            "| epoch  59 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    72.98\n",
            "| epoch  59 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    69.88\n",
            "| epoch  59 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.81\n",
            "| epoch  59 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.21 | ppl    67.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 77.36s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0008382256887853144\n",
            "val_loss is 5.522505760192871\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  60 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    83.87\n",
            "| epoch  60 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.39 | ppl    80.26\n",
            "| epoch  60 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.46\n",
            "| epoch  60 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.46\n",
            "| epoch  60 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.29 | ppl    73.24\n",
            "| epoch  60 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.25 | ppl    69.94\n",
            "| epoch  60 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.21 | ppl    67.57\n",
            "| epoch  60 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.21 | ppl    67.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 77.37s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0007963144043460487\n",
            "val_loss is 5.522508144378662\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  61 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    83.91\n",
            "| epoch  61 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    80.11\n",
            "| epoch  61 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.36\n",
            "| epoch  61 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.16\n",
            "| epoch  61 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.29 | ppl    73.25\n",
            "| epoch  61 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.25 | ppl    69.93\n",
            "| epoch  61 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.21 | ppl    67.68\n",
            "| epoch  61 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 77.38s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0007564986841287463\n",
            "val_loss is 5.5225090980529785\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  62 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    84.17\n",
            "| epoch  62 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.39 | ppl    80.54\n",
            "| epoch  62 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.45\n",
            "| epoch  62 |   800/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.31 | ppl    74.08\n",
            "| epoch  62 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.29 | ppl    73.33\n",
            "| epoch  62 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.03\n",
            "| epoch  62 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.69\n",
            "| epoch  62 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 77.37s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0007186737499223089\n",
            "val_loss is 5.522516250610352\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  63 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    84.02\n",
            "| epoch  63 |   400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.38 | ppl    80.14\n",
            "| epoch  63 |   600/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.34 | ppl    76.34\n",
            "| epoch  63 |   800/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.31 | ppl    74.38\n",
            "| epoch  63 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.29 | ppl    73.30\n",
            "| epoch  63 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.17\n",
            "| epoch  63 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.80\n",
            "| epoch  63 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 77.38s | valid loss  5.52 | valid ppl   250.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0006827400624261934\n",
            "val_loss is 5.522517681121826\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  64 |   200/ 1715 batches | lr 0.00 | ms/batch 40.78 | loss  4.43 | ppl    83.89\n",
            "| epoch  64 |   400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.39 | ppl    80.25\n",
            "| epoch  64 |   600/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.33 | ppl    76.30\n",
            "| epoch  64 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.30 | ppl    74.02\n",
            "| epoch  64 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.29 | ppl    73.19\n",
            "| epoch  64 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.07\n",
            "| epoch  64 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.58\n",
            "| epoch  64 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.21 | ppl    67.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 77.36s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0006486030593048837\n",
            "val_loss is 5.522522449493408\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  65 |   200/ 1715 batches | lr 0.00 | ms/batch 40.92 | loss  4.43 | ppl    83.93\n",
            "| epoch  65 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    79.99\n",
            "| epoch  65 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.46\n",
            "| epoch  65 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.16\n",
            "| epoch  65 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.23\n",
            "| epoch  65 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    69.79\n",
            "| epoch  65 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.56\n",
            "| epoch  65 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.22 | ppl    67.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 77.36s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0006161729063396395\n",
            "val_loss is 5.522530555725098\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  66 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.43 | ppl    84.13\n",
            "| epoch  66 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.11\n",
            "| epoch  66 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.33 | ppl    76.29\n",
            "| epoch  66 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.20\n",
            "| epoch  66 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.32\n",
            "| epoch  66 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    69.91\n",
            "| epoch  66 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.21 | ppl    67.61\n",
            "| epoch  66 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.21 | ppl    67.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 77.33s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0005853642610226575\n",
            "val_loss is 5.522533416748047\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  67 |   200/ 1715 batches | lr 0.00 | ms/batch 40.76 | loss  4.43 | ppl    84.12\n",
            "| epoch  67 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.38 | ppl    80.04\n",
            "| epoch  67 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.59\n",
            "| epoch  67 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.09\n",
            "| epoch  67 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.09\n",
            "| epoch  67 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.25 | ppl    70.04\n",
            "| epoch  67 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.22 | ppl    67.72\n",
            "| epoch  67 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0005560960479715246\n",
            "val_loss is 5.522534370422363\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  68 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    83.98\n",
            "| epoch  68 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.39 | ppl    80.36\n",
            "| epoch  68 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.52\n",
            "| epoch  68 |   800/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.30 | ppl    74.05\n",
            "| epoch  68 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.29 | ppl    73.24\n",
            "| epoch  68 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.25 | ppl    69.99\n",
            "| epoch  68 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.75\n",
            "| epoch  68 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 77.40s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0005282912455729483\n",
            "val_loss is 5.522535800933838\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  69 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    84.14\n",
            "| epoch  69 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.39 | ppl    80.25\n",
            "| epoch  69 |   600/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.33 | ppl    76.19\n",
            "| epoch  69 |   800/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.31 | ppl    74.13\n",
            "| epoch  69 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.29 | ppl    72.92\n",
            "| epoch  69 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.20\n",
            "| epoch  69 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.69\n",
            "| epoch  69 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0005018766832943009\n",
            "val_loss is 5.522535800933838\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  70 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    83.90\n",
            "| epoch  70 |   400/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.38 | ppl    80.16\n",
            "| epoch  70 |   600/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.34 | ppl    76.70\n",
            "| epoch  70 |   800/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.30 | ppl    73.97\n",
            "| epoch  70 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.31\n",
            "| epoch  70 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.11\n",
            "| epoch  70 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.78\n",
            "| epoch  70 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0004767828491295858\n",
            "val_loss is 5.522536277770996\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  71 |   200/ 1715 batches | lr 0.00 | ms/batch 40.88 | loss  4.43 | ppl    83.92\n",
            "| epoch  71 |   400/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.38 | ppl    80.09\n",
            "| epoch  71 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.41\n",
            "| epoch  71 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.30 | ppl    73.83\n",
            "| epoch  71 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.29 | ppl    73.33\n",
            "| epoch  71 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.04\n",
            "| epoch  71 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.22 | ppl    67.71\n",
            "| epoch  71 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 77.36s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0004529437066731065\n",
            "val_loss is 5.522542476654053\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  72 |   200/ 1715 batches | lr 0.00 | ms/batch 40.88 | loss  4.43 | ppl    84.05\n",
            "| epoch  72 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.24\n",
            "| epoch  72 |   600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.34 | ppl    76.74\n",
            "| epoch  72 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.14\n",
            "| epoch  72 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.29 | ppl    73.11\n",
            "| epoch  72 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    70.11\n",
            "| epoch  72 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.62\n",
            "| epoch  72 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.21 | ppl    67.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00043029652133945114\n",
            "val_loss is 5.522544860839844\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  73 |   200/ 1715 batches | lr 0.00 | ms/batch 40.73 | loss  4.43 | ppl    83.88\n",
            "| epoch  73 |   400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.38 | ppl    80.18\n",
            "| epoch  73 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.41\n",
            "| epoch  73 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    73.92\n",
            "| epoch  73 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.29 | ppl    73.17\n",
            "| epoch  73 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.25 | ppl    69.96\n",
            "| epoch  73 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.22 | ppl    67.71\n",
            "| epoch  73 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.22 | ppl    67.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 77.36s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0004087816952724786\n",
            "val_loss is 5.522546291351318\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  74 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    84.02\n",
            "| epoch  74 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.38 | ppl    80.14\n",
            "| epoch  74 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.33 | ppl    76.28\n",
            "| epoch  74 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.40\n",
            "| epoch  74 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.29 | ppl    72.99\n",
            "| epoch  74 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.25 | ppl    69.93\n",
            "| epoch  74 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.22 | ppl    67.70\n",
            "| epoch  74 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00038834261050885464\n",
            "val_loss is 5.522546291351318\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  75 |   200/ 1715 batches | lr 0.00 | ms/batch 40.82 | loss  4.43 | ppl    84.13\n",
            "| epoch  75 |   400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.39 | ppl    80.31\n",
            "| epoch  75 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.52\n",
            "| epoch  75 |   800/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.30 | ppl    73.91\n",
            "| epoch  75 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.29 | ppl    73.14\n",
            "| epoch  75 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.25 | ppl    70.26\n",
            "| epoch  75 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.66\n",
            "| epoch  75 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 77.39s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0003689254799834119\n",
            "val_loss is 5.522548675537109\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  76 |   200/ 1715 batches | lr 0.00 | ms/batch 40.73 | loss  4.43 | ppl    84.01\n",
            "| epoch  76 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.04\n",
            "| epoch  76 |   600/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.34 | ppl    76.54\n",
            "| epoch  76 |   800/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.30 | ppl    74.05\n",
            "| epoch  76 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.30 | ppl    73.41\n",
            "| epoch  76 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    70.18\n",
            "| epoch  76 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.74\n",
            "| epoch  76 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 77.33s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0003504792059842413\n",
            "val_loss is 5.522549629211426\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  77 |   200/ 1715 batches | lr 0.00 | ms/batch 40.83 | loss  4.43 | ppl    84.06\n",
            "| epoch  77 |   400/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.38 | ppl    79.87\n",
            "| epoch  77 |   600/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.33 | ppl    76.23\n",
            "| epoch  77 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.14\n",
            "| epoch  77 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.29 | ppl    73.22\n",
            "| epoch  77 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    69.95\n",
            "| epoch  77 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.82\n",
            "| epoch  77 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 77.35s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0003329552456850292\n",
            "val_loss is 5.522549152374268\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  78 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    83.72\n",
            "| epoch  78 |   400/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.38 | ppl    80.08\n",
            "| epoch  78 |   600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.34 | ppl    76.61\n",
            "| epoch  78 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.31\n",
            "| epoch  78 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.29\n",
            "| epoch  78 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    69.98\n",
            "| epoch  78 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.85\n",
            "| epoch  78 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 77.34s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00031630748340077773\n",
            "val_loss is 5.522552013397217\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  79 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    84.13\n",
            "| epoch  79 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.39 | ppl    80.37\n",
            "| epoch  79 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.33\n",
            "| epoch  79 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.30 | ppl    73.84\n",
            "| epoch  79 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.29 | ppl    73.13\n",
            "| epoch  79 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.25 | ppl    69.81\n",
            "| epoch  79 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.22 | ppl    67.85\n",
            "| epoch  79 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.21 | ppl    67.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00030049210923073884\n",
            "val_loss is 5.522552013397217\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  80 |   200/ 1715 batches | lr 0.00 | ms/batch 40.77 | loss  4.43 | ppl    83.75\n",
            "| epoch  80 |   400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.38 | ppl    80.01\n",
            "| epoch  80 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.35\n",
            "| epoch  80 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.23\n",
            "| epoch  80 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    73.43\n",
            "| epoch  80 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.25 | ppl    70.13\n",
            "| epoch  80 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.22 | ppl    67.81\n",
            "| epoch  80 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.22 | ppl    67.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0002854675037692019\n",
            "val_loss is 5.522552967071533\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  81 |   200/ 1715 batches | lr 0.00 | ms/batch 40.78 | loss  4.43 | ppl    83.81\n",
            "| epoch  81 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    80.04\n",
            "| epoch  81 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.40\n",
            "| epoch  81 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.29\n",
            "| epoch  81 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.29 | ppl    73.32\n",
            "| epoch  81 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.25 | ppl    70.00\n",
            "| epoch  81 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.82\n",
            "| epoch  81 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.22 | ppl    67.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0002711941285807418\n",
            "val_loss is 5.522556781768799\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  82 |   200/ 1715 batches | lr 0.00 | ms/batch 40.78 | loss  4.43 | ppl    83.90\n",
            "| epoch  82 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    80.23\n",
            "| epoch  82 |   600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.34 | ppl    76.52\n",
            "| epoch  82 |   800/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.31 | ppl    74.25\n",
            "| epoch  82 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.29 | ppl    73.24\n",
            "| epoch  82 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.03\n",
            "| epoch  82 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.79\n",
            "| epoch  82 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 77.37s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0002576344221517047\n",
            "val_loss is 5.522558689117432\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  83 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    83.97\n",
            "| epoch  83 |   400/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.38 | ppl    80.17\n",
            "| epoch  83 |   600/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.34 | ppl    76.69\n",
            "| epoch  83 |   800/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.31 | ppl    74.20\n",
            "| epoch  83 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    73.41\n",
            "| epoch  83 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    69.91\n",
            "| epoch  83 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.78\n",
            "| epoch  83 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 77.38s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.0002447527010441195\n",
            "val_loss is 5.522558689117432\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  84 |   200/ 1715 batches | lr 0.00 | ms/batch 40.88 | loss  4.43 | ppl    83.90\n",
            "| epoch  84 |   400/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.38 | ppl    80.02\n",
            "| epoch  84 |   600/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.34 | ppl    76.42\n",
            "| epoch  84 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    74.04\n",
            "| epoch  84 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.09\n",
            "| epoch  84 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.25 | ppl    69.95\n",
            "| epoch  84 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.58\n",
            "| epoch  84 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.21 | ppl    67.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 77.36s | valid loss  5.52 | valid ppl   250.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00023251506599191352\n",
            "val_loss is 5.52255916595459\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  85 |   200/ 1715 batches | lr 0.00 | ms/batch 40.86 | loss  4.43 | ppl    83.94\n",
            "| epoch  85 |   400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.38 | ppl    80.11\n",
            "| epoch  85 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.40\n",
            "| epoch  85 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.30 | ppl    74.06\n",
            "| epoch  85 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.22\n",
            "| epoch  85 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    69.94\n",
            "| epoch  85 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.66\n",
            "| epoch  85 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.21 | ppl    67.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00022088931269231783\n",
            "val_loss is 5.522562503814697\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  86 |   200/ 1715 batches | lr 0.00 | ms/batch 40.79 | loss  4.43 | ppl    83.82\n",
            "| epoch  86 |   400/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.39 | ppl    80.36\n",
            "| epoch  86 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.37\n",
            "| epoch  86 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.31 | ppl    74.15\n",
            "| epoch  86 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.30 | ppl    73.33\n",
            "| epoch  86 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.52 | loss  4.25 | ppl    70.18\n",
            "| epoch  86 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.22 | ppl    67.71\n",
            "| epoch  86 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.67 | loss  4.21 | ppl    67.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 77.36s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00020984484705770194\n",
            "val_loss is 5.5225629806518555\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  87 |   200/ 1715 batches | lr 0.00 | ms/batch 40.76 | loss  4.43 | ppl    83.84\n",
            "| epoch  87 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.12\n",
            "| epoch  87 |   600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.34 | ppl    76.43\n",
            "| epoch  87 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    73.95\n",
            "| epoch  87 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.29\n",
            "| epoch  87 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.25 | ppl    70.25\n",
            "| epoch  87 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.22 | ppl    67.73\n",
            "| epoch  87 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00019935260470481684\n",
            "val_loss is 5.5225629806518555\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  88 |   200/ 1715 batches | lr 0.00 | ms/batch 40.77 | loss  4.43 | ppl    84.19\n",
            "| epoch  88 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.39 | ppl    80.41\n",
            "| epoch  88 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.60\n",
            "| epoch  88 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.18\n",
            "| epoch  88 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.29 | ppl    73.21\n",
            "| epoch  88 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.25 | ppl    70.12\n",
            "| epoch  88 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.69\n",
            "| epoch  88 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00018938497446957598\n",
            "val_loss is 5.52256441116333\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  89 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.43 | ppl    83.81\n",
            "| epoch  89 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.38 | ppl    80.20\n",
            "| epoch  89 |   600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.34 | ppl    76.47\n",
            "| epoch  89 |   800/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.30 | ppl    74.03\n",
            "| epoch  89 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.29 | ppl    73.13\n",
            "| epoch  89 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.12\n",
            "| epoch  89 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.63\n",
            "| epoch  89 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.22 | ppl    67.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 77.36s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00017991572574609717\n",
            "val_loss is 5.522566318511963\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  90 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    83.70\n",
            "| epoch  90 |   400/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.38 | ppl    80.02\n",
            "| epoch  90 |   600/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.34 | ppl    76.56\n",
            "| epoch  90 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.30 | ppl    74.01\n",
            "| epoch  90 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.29 | ppl    73.01\n",
            "| epoch  90 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.11\n",
            "| epoch  90 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.79\n",
            "| epoch  90 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 77.34s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00017091993945879232\n",
            "val_loss is 5.522567272186279\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  91 |   200/ 1715 batches | lr 0.00 | ms/batch 40.85 | loss  4.43 | ppl    84.03\n",
            "| epoch  91 |   400/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.38 | ppl    80.00\n",
            "| epoch  91 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.46\n",
            "| epoch  91 |   800/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.31 | ppl    74.14\n",
            "| epoch  91 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.30 | ppl    73.44\n",
            "| epoch  91 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.05\n",
            "| epoch  91 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.53 | loss  4.21 | ppl    67.58\n",
            "| epoch  91 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 77.33s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00016237394248585268\n",
            "val_loss is 5.522566318511963\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  92 |   200/ 1715 batches | lr 0.00 | ms/batch 40.90 | loss  4.43 | ppl    84.22\n",
            "| epoch  92 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    80.24\n",
            "| epoch  92 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.42\n",
            "| epoch  92 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.26\n",
            "| epoch  92 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.22\n",
            "| epoch  92 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.11\n",
            "| epoch  92 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.49\n",
            "| epoch  92 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.21 | ppl    67.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 77.34s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00015425524536156004\n",
            "val_loss is 5.522567272186279\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  93 |   200/ 1715 batches | lr 0.00 | ms/batch 40.77 | loss  4.43 | ppl    83.82\n",
            "| epoch  93 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    80.03\n",
            "| epoch  93 |   600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.34 | ppl    76.57\n",
            "| epoch  93 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.23\n",
            "| epoch  93 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.52 | loss  4.29 | ppl    73.05\n",
            "| epoch  93 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.25 | ppl    70.00\n",
            "| epoch  93 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.22 | ppl    67.87\n",
            "| epoch  93 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.67 | loss  4.21 | ppl    67.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00014654248309348204\n",
            "val_loss is 5.522568702697754\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  94 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.43 | ppl    84.12\n",
            "| epoch  94 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.38 | ppl    80.15\n",
            "| epoch  94 |   600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.34 | ppl    76.39\n",
            "| epoch  94 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.18\n",
            "| epoch  94 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.29 | ppl    73.03\n",
            "| epoch  94 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.25 | ppl    70.16\n",
            "| epoch  94 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.21 | ppl    67.65\n",
            "| epoch  94 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.22 | ppl    67.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00013921535893880794\n",
            "val_loss is 5.522570610046387\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  95 |   200/ 1715 batches | lr 0.00 | ms/batch 40.79 | loss  4.43 | ppl    84.03\n",
            "| epoch  95 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.39 | ppl    80.45\n",
            "| epoch  95 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.80\n",
            "| epoch  95 |   800/ 1715 batches | lr 0.00 | ms/batch 40.60 | loss  4.30 | ppl    74.00\n",
            "| epoch  95 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.29 | ppl    73.10\n",
            "| epoch  95 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.25 | ppl    69.94\n",
            "| epoch  95 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.21 | ppl    67.69\n",
            "| epoch  95 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.21 | ppl    67.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 77.36s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00013225459099186753\n",
            "val_loss is 5.5225725173950195\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  96 |   200/ 1715 batches | lr 0.00 | ms/batch 40.80 | loss  4.43 | ppl    83.84\n",
            "| epoch  96 |   400/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.38 | ppl    79.91\n",
            "| epoch  96 |   600/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.34 | ppl    76.39\n",
            "| epoch  96 |   800/ 1715 batches | lr 0.00 | ms/batch 40.64 | loss  4.31 | ppl    74.08\n",
            "| epoch  96 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.29 | ppl    73.30\n",
            "| epoch  96 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.25 | ppl    69.84\n",
            "| epoch  96 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.22 | ppl    67.92\n",
            "| epoch  96 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 77.38s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00012564186144227413\n",
            "val_loss is 5.522573471069336\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  97 |   200/ 1715 batches | lr 0.00 | ms/batch 40.75 | loss  4.43 | ppl    83.83\n",
            "| epoch  97 |   400/ 1715 batches | lr 0.00 | ms/batch 40.65 | loss  4.38 | ppl    80.05\n",
            "| epoch  97 |   600/ 1715 batches | lr 0.00 | ms/batch 40.66 | loss  4.34 | ppl    76.33\n",
            "| epoch  97 |   800/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.31 | ppl    74.20\n",
            "| epoch  97 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.29 | ppl    73.30\n",
            "| epoch  97 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.25 | ppl    70.12\n",
            "| epoch  97 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.21 | ppl    67.67\n",
            "| epoch  97 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 77.35s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00011935976837016042\n",
            "val_loss is 5.522576332092285\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  98 |   200/ 1715 batches | lr 0.00 | ms/batch 40.89 | loss  4.43 | ppl    84.03\n",
            "| epoch  98 |   400/ 1715 batches | lr 0.00 | ms/batch 40.62 | loss  4.38 | ppl    80.14\n",
            "| epoch  98 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.55\n",
            "| epoch  98 |   800/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.31 | ppl    74.20\n",
            "| epoch  98 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.30 | ppl    73.42\n",
            "| epoch  98 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    70.30\n",
            "| epoch  98 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.22 | ppl    67.77\n",
            "| epoch  98 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.21 | ppl    67.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 77.35s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00011339177995165239\n",
            "val_loss is 5.522576332092285\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch  99 |   200/ 1715 batches | lr 0.00 | ms/batch 40.81 | loss  4.43 | ppl    84.16\n",
            "| epoch  99 |   400/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.38 | ppl    79.97\n",
            "| epoch  99 |   600/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.34 | ppl    76.45\n",
            "| epoch  99 |   800/ 1715 batches | lr 0.00 | ms/batch 40.54 | loss  4.30 | ppl    74.06\n",
            "| epoch  99 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.29 | ppl    73.32\n",
            "| epoch  99 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.56 | loss  4.25 | ppl    69.98\n",
            "| epoch  99 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.21 | ppl    67.55\n",
            "| epoch  99 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.63 | loss  4.21 | ppl    67.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 77.36s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00010772219095406976\n",
            "val_loss is 5.522578716278076\n",
            "best_val_loss is 5.522489547729492\n",
            "| epoch 100 |   200/ 1715 batches | lr 0.00 | ms/batch 40.78 | loss  4.43 | ppl    83.90\n",
            "| epoch 100 |   400/ 1715 batches | lr 0.00 | ms/batch 40.57 | loss  4.38 | ppl    80.07\n",
            "| epoch 100 |   600/ 1715 batches | lr 0.00 | ms/batch 40.58 | loss  4.33 | ppl    76.31\n",
            "| epoch 100 |   800/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.31 | ppl    74.19\n",
            "| epoch 100 |  1000/ 1715 batches | lr 0.00 | ms/batch 40.55 | loss  4.30 | ppl    73.41\n",
            "| epoch 100 |  1200/ 1715 batches | lr 0.00 | ms/batch 40.61 | loss  4.25 | ppl    70.16\n",
            "| epoch 100 |  1400/ 1715 batches | lr 0.00 | ms/batch 40.68 | loss  4.21 | ppl    67.57\n",
            "| epoch 100 |  1600/ 1715 batches | lr 0.00 | ms/batch 40.59 | loss  4.22 | ppl    67.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 77.37s | valid loss  5.52 | valid ppl   250.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "new learning rate is 0.00010233608140636627\n",
            "val_loss is 5.522578716278076\n",
            "best_val_loss is 5.522489547729492\n",
            "=========================================================================================\n",
            "| End of training | test loss 5.520 | test ppl  249.558\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR-gEbZzaX9R"
      },
      "source": [
        "%tb"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}