{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTkeShnMDvXTiQk/oN4K4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruttikajain/ACL2019-Reducing-Gender-Bias-in-Word-Level-Language-Models-Using-A-Gender-Equalizing-Loss-Function/blob/master/generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "V4MxeyD_kuR0",
        "outputId": "59e8d61b-4cf5-47ae-b84c-9a4ed27c440c"
      },
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "##############################################################################\n",
        "#language Modeling on Penn Tree Bank\n",
        "#\n",
        "# This file generates new sentences sampled from the language model\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import data_v3\n",
        "import json\n",
        "import pandas as pd\n",
        "import preprocess\n",
        "import os\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import random\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch bbc Language Model')\n",
        "\n",
        "# Model parameters.\n",
        "parser.add_argument('--data', type=str, default='./data/preprocessed/data',\n",
        "                    help='location of the data corpus')\n",
        "parser.add_argument('--checkpointpath', type=str, default='./savedmodel',\n",
        "                    help='model checkpoint to use')\n",
        "parser.add_argument('--outDir', type=str, default='./data/generated',\n",
        "                    help='number of output file for generated text')\n",
        "parser.add_argument('--words', type=int, default='500',\n",
        "                    help='words to generate')\n",
        "parser.add_argument('--documents', type=int, default='4000',\n",
        "                    help='number of files to generate')\n",
        "parser.add_argument('--no-sentence-reset', default=False, \n",
        "                    help='do not reset the hidden state in between sentences') #action='store_true',\n",
        "parser.add_argument('--seed', type=int, default=20190331,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--cuda', action='store_true',\n",
        "                    help='use CUDA')\n",
        "parser.add_argument('--temperature', type=float, default=1.0,\n",
        "                    help='temperature - higher will increase diversity')\n",
        "parser.add_argument('--log-interval', type=int, default=100,\n",
        "                    help='reporting interval')\n",
        "parser.add_argument('--emsize', type=int, default=300,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--vocab', type=str, default='./data/preprocessed/VOCAB.txt',\n",
        "                    help=('preprocessed vocaburary'))\n",
        "parser.add_argument('--glove_path', type=str, default='./gn_glove/1b-vectors300-0.8-0.8.txt',\n",
        "                    help='using glove word embedding')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "if args.temperature < 1e-3:\n",
        "    parser.error(\"--temperature has to be greater or equal 1e-3\")\n",
        "\n",
        "\n",
        "#load vocab            \n",
        "vocab = preprocess.read_vocab(os.path.join(args.vocab))\n",
        "if 'cda' in args.vocab:\n",
        "    idx_train = pd.read_json('idx_train_cda.json')\n",
        "    idx_val = pd.read_json('idx_val_cda.json')\n",
        "    idx_test = pd.read_json('idx_test_cda.json')\n",
        "else:\n",
        "    idx_train = pd.read_json('idx_train.json')\n",
        "    idx_val = pd.read_json('idx_val.json')\n",
        "    idx_test = pd.read_json('idx_test.json')\n",
        "\n",
        "# Load pretrained Embeddings, common token of vocab and gn_glove will be loaded, only appear in vocab will be initialized\n",
        "vocab.append('<eos>')\n",
        "ntokens = len(vocab)\n",
        "\n",
        "words2idx = {item : index for index, item in enumerate(vocab)}\n",
        "\n",
        "# Load data\n",
        "corpus = data_v3.Corpus(args.data, vocab, words2idx, idx_train, idx_val, idx_test) #改动2 \n",
        "\n",
        "def generateFile(outf, model):\n",
        "    # torch.initial_seed()\n",
        "    hidden = model.init_hidden(1)\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outf:       \n",
        "        with torch.no_grad():  # no tracking history\n",
        "            for i in range(args.words):\n",
        "                output, hidden = model(input, hidden)\n",
        "                word_weights = output.squeeze().div(args.temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                input.fill_(word_idx)\n",
        "                word = corpus.idx2words[word_idx]\n",
        "\n",
        "                outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "def generateData(checkpoint, model_out_dir):\n",
        "    with open(checkpoint, 'rb') as f:\n",
        "        model = torch.load(f).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(args.documents):\n",
        "        outf = model_out_dir + '/' + str(i+1) + '.txt'\n",
        "        generateFile(outf, model)\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print('| Generated {}/{} documents'.format(i, args.documents))\n",
        "\n",
        "\n",
        "modelDir = args.checkpointpath\n",
        "modelFiles = [m for m in os.listdir(modelDir) if m.endswith('.pt')]\n",
        "\n",
        "\n",
        "#generate files for each saved models\n",
        "outDir = args.outDir\n",
        "if not os.path.isdir(outDir):\n",
        "    os.makedirs(outDir)\n",
        "for m in modelFiles:\n",
        "    print('processing: '+m)\n",
        "    modelName = '.'.join(m.split('.')[:-1])\n",
        "    checkpoint = os.path.join(modelDir,m)\n",
        "    model_out_dir = os.path.join(outDir,modelName)\n",
        "    if not os.path.isdir(model_out_dir):\n",
        "        os.makedirs(model_out_dir)\n",
        "    generateData(checkpoint, model_out_dir)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4b83530b6358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_v3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_v3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}